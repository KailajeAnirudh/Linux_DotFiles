{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9385,
     "status": "ok",
     "timestamp": 1602708097901,
     "user": {
      "displayName": "Rongguang Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqffpTHKBt-HA_j5yVfGYHYav18pCxVQeJK3svUQ=s64",
      "userId": "12257203727687283386"
     },
     "user_tz": 240
    },
    "id": "WB7znsvhyuRH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch and torchvision imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TI6XEXSGyYkh"
   },
   "source": [
    "### (a) Plot the training and validation losses and errors as a function of the number of epochs\n",
    "\n",
    "\n",
    " The model currently does not achieve less than 12% validation error, you have to tweak the parameters to get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "b5e309a9ca7646eb8f5dd635c44cc224",
      "124672c4633e4a2385f45994bd87c8c2",
      "c562c1a7dffa4414a3502c4df2fd0917",
      "d481ffb70dae4d8ca418e21948098208",
      "1a06ccd7b5874511add00fb4ffa13166",
      "27b31369fd9b4931a9931b13e2951126",
      "7d6407337c5049e3a187b039f64d475c",
      "32a6e51d2ddb4ccbba72f8d3e44c7e15"
     ]
    },
    "executionInfo": {
     "elapsed": 7319,
     "status": "ok",
     "timestamp": 1602708108431,
     "user": {
      "displayName": "Rongguang Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqffpTHKBt-HA_j5yVfGYHYav18pCxVQeJK3svUQ=s64",
      "userId": "12257203727687283386"
     },
     "user_tz": 240
    },
    "id": "AaNQQai_wON0",
    "outputId": "b6418d22-d4db-4623-bef7-65b8d67f903a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Reading in the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandAugment(), transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# Defining the model\n",
    "class View(nn.Module):\n",
    "    def __init__(self,o):\n",
    "        super().__init__()\n",
    "        self.o = o\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x.view(-1, self.o)\n",
    "    \n",
    "class allcnn_t(nn.Module):\n",
    "    def __init__(self, c1=96, c2= 192):\n",
    "        super().__init__()\n",
    "        d = 0.5\n",
    "\n",
    "        def convbn(ci,co,ksz,s=1,pz=0):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(co))\n",
    "\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            convbn(3,c1,3,1,1),\n",
    "            convbn(c1,c1,3,1,1),\n",
    "            convbn(c1,c1,3,2,1),\n",
    "            nn.Dropout(d),\n",
    "            convbn(c1,c2,3,1,1),\n",
    "            convbn(c2,c2,3,1,1),\n",
    "            convbn(c2,c2,3,2,1),\n",
    "            nn.Dropout(d),\n",
    "            convbn(c2,c2,3,1,1),\n",
    "            convbn(c2,c2,3,1,1),\n",
    "            convbn(c2,10,1,1),\n",
    "            nn.AvgPool2d(8),\n",
    "            View(10))\n",
    "\n",
    "        print('Num parameters: ', sum([p.numel() for p in self.m.parameters()]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1602708119318,
     "user": {
      "displayName": "Rongguang Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqffpTHKBt-HA_j5yVfGYHYav18pCxVQeJK3svUQ=s64",
      "userId": "12257203727687283386"
     },
     "user_tz": 240
    },
    "id": "34JavUIgRpcP"
   },
   "outputs": [],
   "source": [
    "# The training loop\n",
    "def train(net, optimizer, criterion, train_loader, test_loader, epochs, model_name, plot):\n",
    "    model = net.to(device)\n",
    "    total_step = len(train_loader)\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [40, 80, epochs-20], gamma = 0.1, verbose=True)\n",
    "    overall_step = 0\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        flag = 0\n",
    "        running_loss = 0.0\n",
    "        if epoch == 25 and flag == 0:\n",
    "          for op_params in optimizer.param_groups:\n",
    "            op_params['lr'] = 0.001\n",
    "          flag = 1\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to configured device\n",
    "            images = images.to(device).to(torch.float16)\n",
    "            labels = labels.to(device)\n",
    "            #Forward Pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 1000 == 0:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
    "            if plot:\n",
    "              info = { ('loss_' + model_name): loss.item() }\n",
    "\n",
    "              # for tag, value in info.items():\n",
    "              #   logger.scalar_summary(tag, value, overall_step+1)\n",
    "        train_loss_values.append(running_loss)\n",
    "        train_error.append(100-100*correct/total)\n",
    "        # scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                images = images.to(device).to(torch.float16)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n",
    "        val_error.append(100-100*correct/total)\n",
    "        val_loss_values.append(running_loss)\n",
    "    return val_error,val_loss_values,train_error,train_loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  1667166\n"
     ]
    }
   ],
   "source": [
    "model = allcnn_t().to(device).to(torch.float16)\n",
    "# model = torch.compile(model)\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.00001, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Rand Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load an example image\n",
    "image = None\n",
    "for i, (images, labels) in enumerate(trainloader):\n",
    "    image = images[3].detach().cpu()\n",
    "    break\n",
    "\n",
    "# Display the original image\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(image.permute(1, 2, 0))  # Transpose the dimensions\n",
    "ax.set_title(\"Original\")\n",
    "ax.axis('off')\n",
    "plt.savefig('Original image.png')\n",
    "plt.show()\n",
    "\n",
    "# Define a list of transformations\n",
    "titles = ['Shear Y: (0,30)', 'Shear X: (30, 0)', 'Translate X: 10', \n",
    "          'Translate Y: 10', 'Rotate: 10', 'Brightness: +40%', 'Saturation: +80%', 'Posterize: 5', 'Solarize: 0.5', 'Equalize']\n",
    "transformations = [\n",
    "    F.affine(image, angle = 0, translate=[0,0], scale = 1.0,shear = (0,30), fill = 0),\n",
    "    F.affine(image, angle = 0,  translate=[0,0], scale = 1.0, shear = (30,0), fill = 0),\n",
    "    F.affine(image, angle =0, translate = [10, 0], scale = 1.0, shear=[0, 0]), \n",
    "    F.affine(image, angle=0,translate =[0,10], scale=1.0, shear=[0,0]), \n",
    "    F.rotate(image, angle = 10, fill = 0),\n",
    "    F.adjust_brightness(image, brightness_factor=1.4),\n",
    "    F.adjust_saturation(image, saturation_factor=1.8),\n",
    "    F.posterize((image*255).to(dtype = torch.uint8), bits = 5),\n",
    "    F.solarize(image, threshold=0.5),\n",
    "    F.equalize((image*255).to(dtype = torch.uint8))\n",
    "]\n",
    "\n",
    "# Create a subplot to display the original and augmented images\n",
    "num_rows = 2\n",
    "num_cols = 5\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 9))\n",
    "axes = axes.ravel()\n",
    "axes[0].imshow(image.permute(1, 2, 0))\n",
    "axes[0].set_title(\"Original\")\n",
    "\n",
    "# Apply the augmentations and display the results\n",
    "for i, transform in enumerate(transformations):\n",
    "    augmented_image = transform.numpy().transpose((1,2,0))\n",
    "    axes[i].imshow(augmented_image)\n",
    "    axes[i].set_title(titles[i])\n",
    "\n",
    "# Remove axis labels\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Augmentations.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2120944,
     "status": "ok",
     "timestamp": 1602710244247,
     "user": {
      "displayName": "Rongguang Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqffpTHKBt-HA_j5yVfGYHYav18pCxVQeJK3svUQ=s64",
      "userId": "12257203727687283386"
     },
     "user_tz": 240
    },
    "id": "Y7YvtAlERz61",
    "outputId": "9be29ece-8f71-4f4f-b8e2-77cf6c6fee8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch [1/10], Step [1000/3125], Loss: 2.3613\n",
      "Epoch [1/10], Step [2000/3125], Loss: 1.6113\n",
      "Epoch [1/10], Step [3000/3125], Loss: 1.2422\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Accuracy of the network on the test images: 40.77 %\n",
      "Epoch [2/10], Step [1000/3125], Loss: 1.4785\n",
      "Epoch [2/10], Step [2000/3125], Loss: 1.1758\n",
      "Epoch [2/10], Step [3000/3125], Loss: 1.4326\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Accuracy of the network on the test images: 57.95 %\n",
      "Epoch [3/10], Step [1000/3125], Loss: 1.1338\n",
      "Epoch [3/10], Step [2000/3125], Loss: 0.5991\n",
      "Epoch [3/10], Step [3000/3125], Loss: 1.8477\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Accuracy of the network on the test images: 65.15 %\n",
      "Epoch [4/10], Step [1000/3125], Loss: 1.4316\n",
      "Epoch [4/10], Step [2000/3125], Loss: 0.8384\n",
      "Epoch [4/10], Step [3000/3125], Loss: 1.1006\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Accuracy of the network on the test images: 71.2 %\n",
      "Epoch [5/10], Step [1000/3125], Loss: 1.6348\n",
      "Epoch [5/10], Step [2000/3125], Loss: 0.3406\n",
      "Epoch [5/10], Step [3000/3125], Loss: 1.3164\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Accuracy of the network on the test images: 74.26 %\n",
      "Epoch [6/10], Step [1000/3125], Loss: 1.0967\n",
      "Epoch [6/10], Step [2000/3125], Loss: 0.7578\n",
      "Epoch [6/10], Step [3000/3125], Loss: 1.0996\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Accuracy of the network on the test images: 75.56 %\n",
      "Epoch [7/10], Step [1000/3125], Loss: 0.3147\n",
      "Epoch [7/10], Step [2000/3125], Loss: 0.3259\n",
      "Epoch [7/10], Step [3000/3125], Loss: 0.7266\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Accuracy of the network on the test images: 77.3 %\n",
      "Epoch [8/10], Step [1000/3125], Loss: 0.5698\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m val_error,val_loss_values,train_error,train_loss_values\u001b[39m=\u001b[39m train(model, optimizer, criterion, trainloader, testloader, epochs, \u001b[39m'\u001b[39;49m\u001b[39mcnn_curve\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     op_params[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   flag \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Move tensors to configured device\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat16)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/02_Homework2/hw2_problem3_Anirudh.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchvision/transforms/functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    165\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[0;32m--> 166\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(pic, mode_to_nptype\u001b[39m.\u001b[39mget(pic\u001b[39m.\u001b[39mmode, np\u001b[39m.\u001b[39muint8), copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:691\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    688\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array_interface__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    689\u001b[0m     \u001b[39m# numpy array interface support\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     new \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 691\u001b[0m     shape, typestr \u001b[39m=\u001b[39m _conv_type_shape(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    692\u001b[0m     new[\u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m shape\n\u001b[1;32m    693\u001b[0m     new[\u001b[39m\"\u001b[39m\u001b[39mtypestr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m typestr\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:250\u001b[0m, in \u001b[0;36m_conv_type_shape\u001b[0;34m(im)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_conv_type_shape\u001b[39m(im):\n\u001b[1;32m    249\u001b[0m     m \u001b[39m=\u001b[39m ImageMode\u001b[39m.\u001b[39mgetmode(im\u001b[39m.\u001b[39mmode)\n\u001b[0;32m--> 250\u001b[0m     shape \u001b[39m=\u001b[39m (im\u001b[39m.\u001b[39mheight, im\u001b[39m.\u001b[39;49mwidth)\n\u001b[1;32m    251\u001b[0m     extra \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(m\u001b[39m.\u001b[39mbands)\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m extra \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:531\u001b[0m, in \u001b[0;36mImage.width\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category\n\u001b[1;32m    529\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n\u001b[0;32m--> 531\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwidth\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    536\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mheight\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_error,val_loss_values,train_error,train_loss_values= train(model, optimizer, criterion, trainloader, testloader, epochs, 'cnn_curve', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_error, 'r')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Validation %error')\n",
    "plt.text(len(val_error)-10, val_error[-1], f'{val_error[-1]:.2f}%')\n",
    "plt.savefig('Validation Error.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_error, 'r')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Train %error')\n",
    "plt.text(len(train_error)-10, train_error[-1], f'{train_error[-1]:.2f}%')\n",
    "plt.savefig('Train Error.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(val_loss_values, 'r')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Total Validation loss')\n",
    "plt.text(len(val_loss_values)-10, val_loss_values[-1], f'{val_loss_values[-1]:.2f}')\n",
    "plt.savefig('ValidationLoss.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss_values, 'r')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Total Train loss')\n",
    "plt.text(len(train_loss_values)-10, train_loss_values[-1], f'{train_loss_values[-1]:.2f}')\n",
    "plt.savefig('TrainLoss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Gradients for the the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size =1, shuffle = False)\n",
    "model.eval()\n",
    "correct_images = []\n",
    "incorrect_images = []\n",
    "\n",
    "for i , (images, labels) in enumerate(testloader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    images.requires_grad = True\n",
    "\n",
    "    results = model(images)\n",
    "    loss = criterion(results, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    dx = images.grad.data.clone().to('cpu').detach().numpy()[0]\n",
    "\n",
    "    if torch.argmax(results).item() == labels.item() and len(correct_images)<5:\n",
    "        correct_images.append((images, dx))\n",
    "    elif len(incorrect_images)<5:\n",
    "        incorrect_images.append((images, dx))\n",
    "    else:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_images), len(incorrect_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize = (15, 15.5))\n",
    "for i, (images, dx) in enumerate(correct_images):\n",
    "    # dx = dx.detach().cpu().numpy()[0]\n",
    "    axes[i, 0].imshow(images[0].detach().to('cpu').numpy().transpose((1,2,0)))\n",
    "    axes[i, 0].set_title('Image')\n",
    "    axes[i,1].imshow(dx[0], cmap = 'Reds')\n",
    "    axes[i,1].set_title('Gradient Red Channel')\n",
    "    axes[i,2].imshow(dx[1], cmap = 'Greens')\n",
    "    axes[i,2].set_title('Gradient Green Channel')\n",
    "    axes[i,3].imshow(dx[2], cmap = 'Blues')\n",
    "    axes[i,3].set_title('Gradient Blue Channel')\n",
    "    axes[i,4].imshow(np.linalg.norm(dx, ord = 2, axis = 0), cmap = 'gray')\n",
    "    axes[i,4].set_title('Gradient Vector Sum')\n",
    "plt.suptitle('Correctly Classified Images')\n",
    "plt.subplots_adjust()\n",
    "plt.savefig('CorrectGrad.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize = (15, 15.5))\n",
    "for i, (images, dx) in enumerate(incorrect_images):\n",
    "    # dx = dx[0].detach().cpu().numpy()\n",
    "    axes[i, 0].imshow(images[0].detach().to('cpu').numpy().transpose((1,2,0)))\n",
    "    axes[i, 0].set_title('Image')\n",
    "    axes[i,1].imshow(dx[0], cmap = 'Reds')\n",
    "    axes[i,1].set_title('Gradient Red Channel')\n",
    "    axes[i,2].imshow(dx[1], cmap = 'Greens')\n",
    "    axes[i,2].set_title('Gradient Green Channel')\n",
    "    axes[i,3].imshow(dx[2], cmap = 'Blues')\n",
    "    axes[i,3].set_title('Gradient Blue Channel')\n",
    "    axes[i,4].imshow(np.linalg.norm(dx, ord = 2, axis = 0), cmap = 'gray')\n",
    "    axes[i,4].set_title('Gradient Vector Sum')\n",
    "plt.suptitle('Incorrectly Classified Images')\n",
    "plt.subplots_adjust()\n",
    "plt.savefig('IncorrectGrad.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toperturbImages = None\n",
    "correspondingLabels = None\n",
    "temploader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True)\n",
    "for i, (images, labels) in enumerate(temploader):\n",
    "    toperturbImages = images\n",
    "    correspondingLabels = labels\n",
    "    break\n",
    "\n",
    "print(len(toperturbImages), len(correspondingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PertubationNumber = 5\n",
    "model.eval()\n",
    "losses = []\n",
    "eps = 8\n",
    "toperturbImages =toperturbImages.to(device)\n",
    "correspondingLabels = correspondingLabels.to(device)\n",
    "\n",
    "for i in range(PertubationNumber):\n",
    "    l = 0\n",
    "    for j in range(len(toperturbImages)):\n",
    "        image = toperturbImages[j].unsqueeze(0).to(device)\n",
    "        label = correspondingLabels[j].unsqueeze(0).to(device)\n",
    "        image = torch.autograd.Variable(image)\n",
    "        image.requires_grad = True\n",
    "        result = model(image)\n",
    "        loss = criterion(result, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        dx = image.grad.data.clone()\n",
    "        image = image + eps*torch.sign(dx)\n",
    "        toperturbImages[j] = image[0]\n",
    "        l+= loss.item()\n",
    "    losses.append(l/100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.plot(losses, '.r')\n",
    "plt.xlabel('# of perturbations')\n",
    "plt.ylabel('Loss for 16 images')\n",
    "plt.savefig('LossVPerturb.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact on one-step perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 8\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "correct_perturb = 0\n",
    "total = 0\n",
    "for i, (images, labels) in enumerate(testloader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    images = torch.autograd.Variable(images)\n",
    "    # labels = torch.autograd.Variable(labels)\n",
    "    images.requires_grad = True\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    dx = images.grad.data.clone()\n",
    "    images = images+(eps/255)*torch.sign(dx)\n",
    "\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct_perturb += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n",
    "print('Accuracy of the network on the test images: {} %'.format(100*correct_perturb/total))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PS2_Q5_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "124672c4633e4a2385f45994bd87c8c2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a06ccd7b5874511add00fb4ffa13166": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "27b31369fd9b4931a9931b13e2951126": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32a6e51d2ddb4ccbba72f8d3e44c7e15": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d6407337c5049e3a187b039f64d475c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5e309a9ca7646eb8f5dd635c44cc224": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c562c1a7dffa4414a3502c4df2fd0917",
       "IPY_MODEL_d481ffb70dae4d8ca418e21948098208"
      ],
      "layout": "IPY_MODEL_124672c4633e4a2385f45994bd87c8c2"
     }
    },
    "c562c1a7dffa4414a3502c4df2fd0917": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27b31369fd9b4931a9931b13e2951126",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1a06ccd7b5874511add00fb4ffa13166",
      "value": 1
     }
    },
    "d481ffb70dae4d8ca418e21948098208": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32a6e51d2ddb4ccbba72f8d3e44c7e15",
      "placeholder": "​",
      "style": "IPY_MODEL_7d6407337c5049e3a187b039f64d475c",
      "value": " 170500096/? [00:20&lt;00:00, 53499326.93it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
