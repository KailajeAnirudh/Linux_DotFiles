{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch and torchvision imports\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    "import numpy as np,  matplotlib.pyplot as plt, pandas as pd\n",
    "from ResnetModel import *\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(np.transpose(np.load('X_train.npy'), axes = (0,2,1))).float()\n",
    "X_test = torch.from_numpy(np.transpose(np.load('X_test.npy'), axes = (0,2,1))).float()\n",
    "y_train = pd.read_pickle('y_train.pickle').to_numpy()\n",
    "y_test = pd.read_pickle('y_test.pickle').to_numpy()\n",
    "\n",
    "class_list = []\n",
    "for classes in y_train:\n",
    "    class_list += classes \n",
    "class_list = set(class_list)\n",
    "class_list\n",
    "diagSupclassDict = {val:i for i, val in enumerate(class_list)}\n",
    "diagSupclassDict['Nodiag'] = 5\n",
    "print(diagSupclassDict)\n",
    "\n",
    "train_label_mapping = torch.zeros((X_train.shape[0], len(diagSupclassDict)))\n",
    "print(f\"-\"*(1+30+5+35))\n",
    "for i, classes in enumerate(y_train):\n",
    "    for diagclass in classes:\n",
    "        train_label_mapping[i, diagSupclassDict[diagclass]] = 1\n",
    "    if len(classes) == 0:\n",
    "        train_label_mapping[i, diagSupclassDict['Nodiag']] = 1\n",
    "    \n",
    "    print(f\"|  {str(y_train[i]):>30}  |  {str(train_label_mapping[i]):<30}   |\")\n",
    "\n",
    "test_label_mapping = torch.zeros((X_test.shape[0], len(diagSupclassDict)))\n",
    "print(f\"-\"*(1+30+5+35))\n",
    "for i, classes in enumerate(y_test):\n",
    "    for diagclass in classes:\n",
    "        test_label_mapping[i, diagSupclassDict[diagclass]] = 1\n",
    "    if len(classes) == 0:\n",
    "        test_label_mapping[i, diagSupclassDict['Nodiag']] = 1\n",
    "    \n",
    "    print(f\"|  {str(y_train[i]):>30}  |  {str(test_label_mapping[i]):<30}   |\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, train_label_mapping)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, test_label_mapping)\n",
    "x = X_train[0:1]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 64, 500]           5,376\n",
      "              ReLU-2              [-1, 64, 500]               0\n",
      "       BatchNorm1d-3              [-1, 64, 500]             128\n",
      "         MaxPool1d-4              [-1, 64, 250]               0\n",
      "            Conv1d-5              [-1, 64, 250]           4,096\n",
      "              ReLU-6              [-1, 64, 250]               0\n",
      "       BatchNorm1d-7              [-1, 64, 250]             128\n",
      "            Conv1d-8              [-1, 64, 250]          12,288\n",
      "              ReLU-9              [-1, 64, 250]               0\n",
      "      BatchNorm1d-10              [-1, 64, 250]             128\n",
      "           Conv1d-11             [-1, 256, 250]          16,384\n",
      "      BatchNorm1d-12             [-1, 256, 250]             512\n",
      "           Conv1d-13             [-1, 256, 250]          16,384\n",
      "      BatchNorm1d-14             [-1, 256, 250]             512\n",
      "             ReLU-15             [-1, 256, 250]               0\n",
      "       Bottleneck-16             [-1, 256, 250]               0\n",
      "           Conv1d-17              [-1, 64, 250]          16,384\n",
      "             ReLU-18              [-1, 64, 250]               0\n",
      "      BatchNorm1d-19              [-1, 64, 250]             128\n",
      "           Conv1d-20              [-1, 64, 250]          12,288\n",
      "             ReLU-21              [-1, 64, 250]               0\n",
      "      BatchNorm1d-22              [-1, 64, 250]             128\n",
      "           Conv1d-23             [-1, 256, 250]          16,384\n",
      "      BatchNorm1d-24             [-1, 256, 250]             512\n",
      "             ReLU-25             [-1, 256, 250]               0\n",
      "       Bottleneck-26             [-1, 256, 250]               0\n",
      "           Conv1d-27              [-1, 64, 250]          16,384\n",
      "             ReLU-28              [-1, 64, 250]               0\n",
      "      BatchNorm1d-29              [-1, 64, 250]             128\n",
      "           Conv1d-30              [-1, 64, 250]          12,288\n",
      "             ReLU-31              [-1, 64, 250]               0\n",
      "      BatchNorm1d-32              [-1, 64, 250]             128\n",
      "           Conv1d-33             [-1, 256, 250]          16,384\n",
      "      BatchNorm1d-34             [-1, 256, 250]             512\n",
      "             ReLU-35             [-1, 256, 250]               0\n",
      "       Bottleneck-36             [-1, 256, 250]               0\n",
      "           Conv1d-37             [-1, 128, 250]          32,768\n",
      "             ReLU-38             [-1, 128, 250]               0\n",
      "      BatchNorm1d-39             [-1, 128, 250]             256\n",
      "           Conv1d-40             [-1, 128, 125]          49,152\n",
      "             ReLU-41             [-1, 128, 125]               0\n",
      "      BatchNorm1d-42             [-1, 128, 125]             256\n",
      "           Conv1d-43             [-1, 512, 125]          65,536\n",
      "      BatchNorm1d-44             [-1, 512, 125]           1,024\n",
      "           Conv1d-45             [-1, 512, 125]         131,072\n",
      "      BatchNorm1d-46             [-1, 512, 125]           1,024\n",
      "             ReLU-47             [-1, 512, 125]               0\n",
      "       Bottleneck-48             [-1, 512, 125]               0\n",
      "           Conv1d-49             [-1, 128, 125]          65,536\n",
      "             ReLU-50             [-1, 128, 125]               0\n",
      "      BatchNorm1d-51             [-1, 128, 125]             256\n",
      "           Conv1d-52             [-1, 128, 125]          49,152\n",
      "             ReLU-53             [-1, 128, 125]               0\n",
      "      BatchNorm1d-54             [-1, 128, 125]             256\n",
      "           Conv1d-55             [-1, 512, 125]          65,536\n",
      "      BatchNorm1d-56             [-1, 512, 125]           1,024\n",
      "             ReLU-57             [-1, 512, 125]               0\n",
      "       Bottleneck-58             [-1, 512, 125]               0\n",
      "           Conv1d-59             [-1, 128, 125]          65,536\n",
      "             ReLU-60             [-1, 128, 125]               0\n",
      "      BatchNorm1d-61             [-1, 128, 125]             256\n",
      "           Conv1d-62             [-1, 128, 125]          49,152\n",
      "             ReLU-63             [-1, 128, 125]               0\n",
      "      BatchNorm1d-64             [-1, 128, 125]             256\n",
      "           Conv1d-65             [-1, 512, 125]          65,536\n",
      "      BatchNorm1d-66             [-1, 512, 125]           1,024\n",
      "             ReLU-67             [-1, 512, 125]               0\n",
      "       Bottleneck-68             [-1, 512, 125]               0\n",
      "           Conv1d-69             [-1, 128, 125]          65,536\n",
      "             ReLU-70             [-1, 128, 125]               0\n",
      "      BatchNorm1d-71             [-1, 128, 125]             256\n",
      "           Conv1d-72             [-1, 128, 125]          49,152\n",
      "             ReLU-73             [-1, 128, 125]               0\n",
      "      BatchNorm1d-74             [-1, 128, 125]             256\n",
      "           Conv1d-75             [-1, 512, 125]          65,536\n",
      "      BatchNorm1d-76             [-1, 512, 125]           1,024\n",
      "             ReLU-77             [-1, 512, 125]               0\n",
      "       Bottleneck-78             [-1, 512, 125]               0\n",
      "           Conv1d-79             [-1, 256, 125]         131,072\n",
      "             ReLU-80             [-1, 256, 125]               0\n",
      "      BatchNorm1d-81             [-1, 256, 125]             512\n",
      "           Conv1d-82              [-1, 256, 63]         196,608\n",
      "             ReLU-83              [-1, 256, 63]               0\n",
      "      BatchNorm1d-84              [-1, 256, 63]             512\n",
      "           Conv1d-85             [-1, 1024, 63]         262,144\n",
      "      BatchNorm1d-86             [-1, 1024, 63]           2,048\n",
      "           Conv1d-87             [-1, 1024, 63]         524,288\n",
      "      BatchNorm1d-88             [-1, 1024, 63]           2,048\n",
      "             ReLU-89             [-1, 1024, 63]               0\n",
      "       Bottleneck-90             [-1, 1024, 63]               0\n",
      "           Conv1d-91              [-1, 256, 63]         262,144\n",
      "             ReLU-92              [-1, 256, 63]               0\n",
      "      BatchNorm1d-93              [-1, 256, 63]             512\n",
      "           Conv1d-94              [-1, 256, 63]         196,608\n",
      "             ReLU-95              [-1, 256, 63]               0\n",
      "      BatchNorm1d-96              [-1, 256, 63]             512\n",
      "           Conv1d-97             [-1, 1024, 63]         262,144\n",
      "      BatchNorm1d-98             [-1, 1024, 63]           2,048\n",
      "             ReLU-99             [-1, 1024, 63]               0\n",
      "      Bottleneck-100             [-1, 1024, 63]               0\n",
      "          Conv1d-101              [-1, 256, 63]         262,144\n",
      "            ReLU-102              [-1, 256, 63]               0\n",
      "     BatchNorm1d-103              [-1, 256, 63]             512\n",
      "          Conv1d-104              [-1, 256, 63]         196,608\n",
      "            ReLU-105              [-1, 256, 63]               0\n",
      "     BatchNorm1d-106              [-1, 256, 63]             512\n",
      "          Conv1d-107             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-108             [-1, 1024, 63]           2,048\n",
      "            ReLU-109             [-1, 1024, 63]               0\n",
      "      Bottleneck-110             [-1, 1024, 63]               0\n",
      "          Conv1d-111              [-1, 256, 63]         262,144\n",
      "            ReLU-112              [-1, 256, 63]               0\n",
      "     BatchNorm1d-113              [-1, 256, 63]             512\n",
      "          Conv1d-114              [-1, 256, 63]         196,608\n",
      "            ReLU-115              [-1, 256, 63]               0\n",
      "     BatchNorm1d-116              [-1, 256, 63]             512\n",
      "          Conv1d-117             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-118             [-1, 1024, 63]           2,048\n",
      "            ReLU-119             [-1, 1024, 63]               0\n",
      "      Bottleneck-120             [-1, 1024, 63]               0\n",
      "          Conv1d-121              [-1, 256, 63]         262,144\n",
      "            ReLU-122              [-1, 256, 63]               0\n",
      "     BatchNorm1d-123              [-1, 256, 63]             512\n",
      "          Conv1d-124              [-1, 256, 63]         196,608\n",
      "            ReLU-125              [-1, 256, 63]               0\n",
      "     BatchNorm1d-126              [-1, 256, 63]             512\n",
      "          Conv1d-127             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-128             [-1, 1024, 63]           2,048\n",
      "            ReLU-129             [-1, 1024, 63]               0\n",
      "      Bottleneck-130             [-1, 1024, 63]               0\n",
      "          Conv1d-131              [-1, 256, 63]         262,144\n",
      "            ReLU-132              [-1, 256, 63]               0\n",
      "     BatchNorm1d-133              [-1, 256, 63]             512\n",
      "          Conv1d-134              [-1, 256, 63]         196,608\n",
      "            ReLU-135              [-1, 256, 63]               0\n",
      "     BatchNorm1d-136              [-1, 256, 63]             512\n",
      "          Conv1d-137             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-138             [-1, 1024, 63]           2,048\n",
      "            ReLU-139             [-1, 1024, 63]               0\n",
      "      Bottleneck-140             [-1, 1024, 63]               0\n",
      "          Conv1d-141              [-1, 256, 63]         262,144\n",
      "            ReLU-142              [-1, 256, 63]               0\n",
      "     BatchNorm1d-143              [-1, 256, 63]             512\n",
      "          Conv1d-144              [-1, 256, 63]         196,608\n",
      "            ReLU-145              [-1, 256, 63]               0\n",
      "     BatchNorm1d-146              [-1, 256, 63]             512\n",
      "          Conv1d-147             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-148             [-1, 1024, 63]           2,048\n",
      "            ReLU-149             [-1, 1024, 63]               0\n",
      "      Bottleneck-150             [-1, 1024, 63]               0\n",
      "          Conv1d-151              [-1, 256, 63]         262,144\n",
      "            ReLU-152              [-1, 256, 63]               0\n",
      "     BatchNorm1d-153              [-1, 256, 63]             512\n",
      "          Conv1d-154              [-1, 256, 63]         196,608\n",
      "            ReLU-155              [-1, 256, 63]               0\n",
      "     BatchNorm1d-156              [-1, 256, 63]             512\n",
      "          Conv1d-157             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-158             [-1, 1024, 63]           2,048\n",
      "            ReLU-159             [-1, 1024, 63]               0\n",
      "      Bottleneck-160             [-1, 1024, 63]               0\n",
      "          Conv1d-161              [-1, 256, 63]         262,144\n",
      "            ReLU-162              [-1, 256, 63]               0\n",
      "     BatchNorm1d-163              [-1, 256, 63]             512\n",
      "          Conv1d-164              [-1, 256, 63]         196,608\n",
      "            ReLU-165              [-1, 256, 63]               0\n",
      "     BatchNorm1d-166              [-1, 256, 63]             512\n",
      "          Conv1d-167             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-168             [-1, 1024, 63]           2,048\n",
      "            ReLU-169             [-1, 1024, 63]               0\n",
      "      Bottleneck-170             [-1, 1024, 63]               0\n",
      "          Conv1d-171              [-1, 256, 63]         262,144\n",
      "            ReLU-172              [-1, 256, 63]               0\n",
      "     BatchNorm1d-173              [-1, 256, 63]             512\n",
      "          Conv1d-174              [-1, 256, 63]         196,608\n",
      "            ReLU-175              [-1, 256, 63]               0\n",
      "     BatchNorm1d-176              [-1, 256, 63]             512\n",
      "          Conv1d-177             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-178             [-1, 1024, 63]           2,048\n",
      "            ReLU-179             [-1, 1024, 63]               0\n",
      "      Bottleneck-180             [-1, 1024, 63]               0\n",
      "          Conv1d-181              [-1, 256, 63]         262,144\n",
      "            ReLU-182              [-1, 256, 63]               0\n",
      "     BatchNorm1d-183              [-1, 256, 63]             512\n",
      "          Conv1d-184              [-1, 256, 63]         196,608\n",
      "            ReLU-185              [-1, 256, 63]               0\n",
      "     BatchNorm1d-186              [-1, 256, 63]             512\n",
      "          Conv1d-187             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-188             [-1, 1024, 63]           2,048\n",
      "            ReLU-189             [-1, 1024, 63]               0\n",
      "      Bottleneck-190             [-1, 1024, 63]               0\n",
      "          Conv1d-191              [-1, 256, 63]         262,144\n",
      "            ReLU-192              [-1, 256, 63]               0\n",
      "     BatchNorm1d-193              [-1, 256, 63]             512\n",
      "          Conv1d-194              [-1, 256, 63]         196,608\n",
      "            ReLU-195              [-1, 256, 63]               0\n",
      "     BatchNorm1d-196              [-1, 256, 63]             512\n",
      "          Conv1d-197             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-198             [-1, 1024, 63]           2,048\n",
      "            ReLU-199             [-1, 1024, 63]               0\n",
      "      Bottleneck-200             [-1, 1024, 63]               0\n",
      "          Conv1d-201              [-1, 256, 63]         262,144\n",
      "            ReLU-202              [-1, 256, 63]               0\n",
      "     BatchNorm1d-203              [-1, 256, 63]             512\n",
      "          Conv1d-204              [-1, 256, 63]         196,608\n",
      "            ReLU-205              [-1, 256, 63]               0\n",
      "     BatchNorm1d-206              [-1, 256, 63]             512\n",
      "          Conv1d-207             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-208             [-1, 1024, 63]           2,048\n",
      "            ReLU-209             [-1, 1024, 63]               0\n",
      "      Bottleneck-210             [-1, 1024, 63]               0\n",
      "          Conv1d-211              [-1, 256, 63]         262,144\n",
      "            ReLU-212              [-1, 256, 63]               0\n",
      "     BatchNorm1d-213              [-1, 256, 63]             512\n",
      "          Conv1d-214              [-1, 256, 63]         196,608\n",
      "            ReLU-215              [-1, 256, 63]               0\n",
      "     BatchNorm1d-216              [-1, 256, 63]             512\n",
      "          Conv1d-217             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-218             [-1, 1024, 63]           2,048\n",
      "            ReLU-219             [-1, 1024, 63]               0\n",
      "      Bottleneck-220             [-1, 1024, 63]               0\n",
      "          Conv1d-221              [-1, 256, 63]         262,144\n",
      "            ReLU-222              [-1, 256, 63]               0\n",
      "     BatchNorm1d-223              [-1, 256, 63]             512\n",
      "          Conv1d-224              [-1, 256, 63]         196,608\n",
      "            ReLU-225              [-1, 256, 63]               0\n",
      "     BatchNorm1d-226              [-1, 256, 63]             512\n",
      "          Conv1d-227             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-228             [-1, 1024, 63]           2,048\n",
      "            ReLU-229             [-1, 1024, 63]               0\n",
      "      Bottleneck-230             [-1, 1024, 63]               0\n",
      "          Conv1d-231              [-1, 256, 63]         262,144\n",
      "            ReLU-232              [-1, 256, 63]               0\n",
      "     BatchNorm1d-233              [-1, 256, 63]             512\n",
      "          Conv1d-234              [-1, 256, 63]         196,608\n",
      "            ReLU-235              [-1, 256, 63]               0\n",
      "     BatchNorm1d-236              [-1, 256, 63]             512\n",
      "          Conv1d-237             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-238             [-1, 1024, 63]           2,048\n",
      "            ReLU-239             [-1, 1024, 63]               0\n",
      "      Bottleneck-240             [-1, 1024, 63]               0\n",
      "          Conv1d-241              [-1, 256, 63]         262,144\n",
      "            ReLU-242              [-1, 256, 63]               0\n",
      "     BatchNorm1d-243              [-1, 256, 63]             512\n",
      "          Conv1d-244              [-1, 256, 63]         196,608\n",
      "            ReLU-245              [-1, 256, 63]               0\n",
      "     BatchNorm1d-246              [-1, 256, 63]             512\n",
      "          Conv1d-247             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-248             [-1, 1024, 63]           2,048\n",
      "            ReLU-249             [-1, 1024, 63]               0\n",
      "      Bottleneck-250             [-1, 1024, 63]               0\n",
      "          Conv1d-251              [-1, 256, 63]         262,144\n",
      "            ReLU-252              [-1, 256, 63]               0\n",
      "     BatchNorm1d-253              [-1, 256, 63]             512\n",
      "          Conv1d-254              [-1, 256, 63]         196,608\n",
      "            ReLU-255              [-1, 256, 63]               0\n",
      "     BatchNorm1d-256              [-1, 256, 63]             512\n",
      "          Conv1d-257             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-258             [-1, 1024, 63]           2,048\n",
      "            ReLU-259             [-1, 1024, 63]               0\n",
      "      Bottleneck-260             [-1, 1024, 63]               0\n",
      "          Conv1d-261              [-1, 256, 63]         262,144\n",
      "            ReLU-262              [-1, 256, 63]               0\n",
      "     BatchNorm1d-263              [-1, 256, 63]             512\n",
      "          Conv1d-264              [-1, 256, 63]         196,608\n",
      "            ReLU-265              [-1, 256, 63]               0\n",
      "     BatchNorm1d-266              [-1, 256, 63]             512\n",
      "          Conv1d-267             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-268             [-1, 1024, 63]           2,048\n",
      "            ReLU-269             [-1, 1024, 63]               0\n",
      "      Bottleneck-270             [-1, 1024, 63]               0\n",
      "          Conv1d-271              [-1, 256, 63]         262,144\n",
      "            ReLU-272              [-1, 256, 63]               0\n",
      "     BatchNorm1d-273              [-1, 256, 63]             512\n",
      "          Conv1d-274              [-1, 256, 63]         196,608\n",
      "            ReLU-275              [-1, 256, 63]               0\n",
      "     BatchNorm1d-276              [-1, 256, 63]             512\n",
      "          Conv1d-277             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-278             [-1, 1024, 63]           2,048\n",
      "            ReLU-279             [-1, 1024, 63]               0\n",
      "      Bottleneck-280             [-1, 1024, 63]               0\n",
      "          Conv1d-281              [-1, 256, 63]         262,144\n",
      "            ReLU-282              [-1, 256, 63]               0\n",
      "     BatchNorm1d-283              [-1, 256, 63]             512\n",
      "          Conv1d-284              [-1, 256, 63]         196,608\n",
      "            ReLU-285              [-1, 256, 63]               0\n",
      "     BatchNorm1d-286              [-1, 256, 63]             512\n",
      "          Conv1d-287             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-288             [-1, 1024, 63]           2,048\n",
      "            ReLU-289             [-1, 1024, 63]               0\n",
      "      Bottleneck-290             [-1, 1024, 63]               0\n",
      "          Conv1d-291              [-1, 256, 63]         262,144\n",
      "            ReLU-292              [-1, 256, 63]               0\n",
      "     BatchNorm1d-293              [-1, 256, 63]             512\n",
      "          Conv1d-294              [-1, 256, 63]         196,608\n",
      "            ReLU-295              [-1, 256, 63]               0\n",
      "     BatchNorm1d-296              [-1, 256, 63]             512\n",
      "          Conv1d-297             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-298             [-1, 1024, 63]           2,048\n",
      "            ReLU-299             [-1, 1024, 63]               0\n",
      "      Bottleneck-300             [-1, 1024, 63]               0\n",
      "          Conv1d-301              [-1, 256, 63]         262,144\n",
      "            ReLU-302              [-1, 256, 63]               0\n",
      "     BatchNorm1d-303              [-1, 256, 63]             512\n",
      "          Conv1d-304              [-1, 256, 63]         196,608\n",
      "            ReLU-305              [-1, 256, 63]               0\n",
      "     BatchNorm1d-306              [-1, 256, 63]             512\n",
      "          Conv1d-307             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-308             [-1, 1024, 63]           2,048\n",
      "            ReLU-309             [-1, 1024, 63]               0\n",
      "      Bottleneck-310             [-1, 1024, 63]               0\n",
      "          Conv1d-311              [-1, 512, 63]         524,288\n",
      "            ReLU-312              [-1, 512, 63]               0\n",
      "     BatchNorm1d-313              [-1, 512, 63]           1,024\n",
      "          Conv1d-314              [-1, 512, 32]         786,432\n",
      "            ReLU-315              [-1, 512, 32]               0\n",
      "     BatchNorm1d-316              [-1, 512, 32]           1,024\n",
      "          Conv1d-317             [-1, 2048, 32]       1,048,576\n",
      "     BatchNorm1d-318             [-1, 2048, 32]           4,096\n",
      "          Conv1d-319             [-1, 2048, 32]       2,097,152\n",
      "     BatchNorm1d-320             [-1, 2048, 32]           4,096\n",
      "            ReLU-321             [-1, 2048, 32]               0\n",
      "      Bottleneck-322             [-1, 2048, 32]               0\n",
      "          Conv1d-323              [-1, 512, 32]       1,048,576\n",
      "            ReLU-324              [-1, 512, 32]               0\n",
      "     BatchNorm1d-325              [-1, 512, 32]           1,024\n",
      "          Conv1d-326              [-1, 512, 32]         786,432\n",
      "            ReLU-327              [-1, 512, 32]               0\n",
      "     BatchNorm1d-328              [-1, 512, 32]           1,024\n",
      "          Conv1d-329             [-1, 2048, 32]       1,048,576\n",
      "     BatchNorm1d-330             [-1, 2048, 32]           4,096\n",
      "            ReLU-331             [-1, 2048, 32]               0\n",
      "      Bottleneck-332             [-1, 2048, 32]               0\n",
      "          Conv1d-333              [-1, 512, 32]       1,048,576\n",
      "            ReLU-334              [-1, 512, 32]               0\n",
      "     BatchNorm1d-335              [-1, 512, 32]           1,024\n",
      "          Conv1d-336              [-1, 512, 32]         786,432\n",
      "            ReLU-337              [-1, 512, 32]               0\n",
      "     BatchNorm1d-338              [-1, 512, 32]           1,024\n",
      "          Conv1d-339             [-1, 2048, 32]       1,048,576\n",
      "     BatchNorm1d-340             [-1, 2048, 32]           4,096\n",
      "            ReLU-341             [-1, 2048, 32]               0\n",
      "      Bottleneck-342             [-1, 2048, 32]               0\n",
      "AdaptiveAvgPool1d-343              [-1, 2048, 1]               0\n",
      "          Linear-344                    [-1, 6]          12,294\n",
      "         Sigmoid-345                    [-1, 6]               0\n",
      "================================================================\n",
      "Total params: 28,278,918\n",
      "Trainable params: 28,278,918\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 95.21\n",
      "Params size (MB): 107.88\n",
      "Estimated Total Size (MB): 203.13\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=6).float()\n",
    "print(summary(model.to(device), (12,1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(4*1024-107.88)/107.88 #Max batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudhkailaje/.local/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1481, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Test AUC metric\"\"\"\n",
    "ml_auroc = MultilabelAUROC(num_labels=len(diagSupclassDict), average=\"macro\", thresholds=None)\n",
    "ml_auroc(model(X_train[0:10].to(device)), train_label_mapping[0:10].to(device).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Max Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "epochs = 10\n",
    "model = model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=6).float()\n",
    "model = model.to(device)\n",
    "lr = 1e-6\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "lrs = []\n",
    "\n",
    "for i, (signal, labels) in enumerate(train_loader):\n",
    "    signal = signal.to(device)\n",
    "    labels = labels.to(device)\n",
    "    output = model(signal)\n",
    "    loss = criterion(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    train_loss.append(loss.item())\n",
    "    lrs.append(lr)\n",
    "    lr *= 1.1\n",
    "\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr \n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if i > 100 or lr > 1:\n",
    "        break\n",
    "\n",
    "lrs = np.array(lrs)\n",
    "train_loss = np.array(train_loss)\n",
    "\n",
    "lr_max = lrs[np.where(train_loss == train_loss.min())[0]]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(lrs, train_loss)\n",
    "plt.plot(lr_max, train_loss[lrs == lr_max], '.r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max = 0.0005/10\n",
    "lr = lr_max\n",
    "epochs = 10\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 10\n",
    "model = model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=6).float()\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay=1e-4)\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/03_Project/src/CNN_Trial.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/03_Project/src/CNN_Trial.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/03_Project/src/CNN_Trial.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m t\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/03_Project/src/CNN_Trial.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train_AUC \u001b[39m=\u001b[39m ml_auroc(outputs, labels\u001b[39m.\u001b[39;49mint())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/03_Project/src/CNN_Trial.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mTrain_Loss\u001b[39m\u001b[39m\"\u001b[39m, loss, t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/03_Project/src/CNN_Trial.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mLearning rate\u001b[39m\u001b[39m\"\u001b[39m, lr, t)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchmetrics/metric.py:298\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_full_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_reduce_state_update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    300\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchmetrics/metric.py:367\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    368\u001b[0m batch_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute()\n\u001b[1;32m    370\u001b[0m \u001b[39m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchmetrics/metric.py:457\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[1;32m    456\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 457\u001b[0m         update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    458\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    459\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchmetrics/classification/precision_recall_curve.py:533\u001b[0m, in \u001b[0;36mMultilabelPrecisionRecallCurve.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Update metric states.\"\"\"\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 533\u001b[0m     _multilabel_precision_recall_curve_tensor_validation(preds, target, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_labels, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index)\n\u001b[1;32m    534\u001b[0m preds, target, _ \u001b[39m=\u001b[39m _multilabel_precision_recall_curve_format(\n\u001b[1;32m    535\u001b[0m     preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthresholds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    537\u001b[0m state \u001b[39m=\u001b[39m _multilabel_precision_recall_curve_update(preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthresholds)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchmetrics/functional/classification/precision_recall_curve.py:681\u001b[0m, in \u001b[0;36m_multilabel_precision_recall_curve_tensor_validation\u001b[0;34m(preds, target, num_labels, ignore_index)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_multilabel_precision_recall_curve_tensor_validation\u001b[39m(\n\u001b[1;32m    671\u001b[0m     preds: Tensor, target: Tensor, num_labels: \u001b[39mint\u001b[39m, ignore_index: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    672\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    673\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate tensor input.\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \n\u001b[1;32m    675\u001b[0m \u001b[39m    - tensors have to be of same shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \n\u001b[1;32m    680\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m     _binary_precision_recall_curve_tensor_validation(preds, target, ignore_index)\n\u001b[1;32m    682\u001b[0m     \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m num_labels:\n\u001b[1;32m    683\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    684\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected both `target.shape[1]` and `preds.shape[1]` to be equal to the number of labels\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    685\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mpreds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m and expected \u001b[39m\u001b[39m{\u001b[39;00mnum_labels\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchmetrics/functional/classification/precision_recall_curve.py:150\u001b[0m, in \u001b[0;36m_binary_precision_recall_curve_tensor_validation\u001b[0;34m(preds, target, ignore_index)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected argument `preds` to be an floating tensor with probability/logit scores,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got tensor with dtype \u001b[39m\u001b[39m{\u001b[39;00mpreds\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    149\u001b[0m \u001b[39m# Check that target only contains {0,1} values or value in ignore_index\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m unique_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49munique(target)\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m ignore_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     check \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39many((unique_values \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (unique_values \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/functional.py:976\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m    974\u001b[0m     \u001b[39mreturn\u001b[39;00m _unique_impl(\u001b[39minput\u001b[39m, \u001b[39msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 976\u001b[0m output, _, _ \u001b[39m=\u001b[39m _unique_impl(\u001b[39minput\u001b[39;49m, \u001b[39msorted\u001b[39;49m, return_inverse, return_counts, dim)\n\u001b[1;32m    977\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/functional.py:890\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    882\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39munique_dim(\n\u001b[1;32m    883\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    884\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         return_counts\u001b[39m=\u001b[39mreturn_counts,\n\u001b[1;32m    888\u001b[0m     )\n\u001b[1;32m    889\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_unique2(\n\u001b[1;32m    891\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    892\u001b[0m         \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m,\n\u001b[1;32m    893\u001b[0m         return_inverse\u001b[39m=\u001b[39;49mreturn_inverse,\n\u001b[1;32m    894\u001b[0m         return_counts\u001b[39m=\u001b[39;49mreturn_counts,\n\u001b[1;32m    895\u001b[0m     )\n\u001b[1;32m    896\u001b[0m \u001b[39mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "steps_per_epoch = len(train_loader)\n",
    "T_max = steps_per_epoch*epochs\n",
    "T_0 = T_max/5 \n",
    "learning_rates = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (signal, labels) in enumerate(train_loader):\n",
    "        signal = signal.to(device); labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signal)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        if t <= T_0:\n",
    "            lr = 10**(-4) + (t/T_0)*lr_max  \n",
    "        else: \n",
    "            lr = lr_max*np.cos((np.pi/2)*((t-T_0)/(T_max-T_0))) + 10**(-6) \n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr \n",
    "        learning_rates.append(lr)\n",
    "        optimizer.step()\n",
    "        t+=1\n",
    "        \n",
    "        train_AUC = ml_auroc(outputs, labels.int())\n",
    "        writer.add_scalar(\"Train_Loss\", loss, t)\n",
    "        writer.add_scalar(\"Learning rate\", lr, t)\n",
    "        writer.add_scalar(\"Batch Train AUC\", train_AUC, t)\n",
    "\n",
    "    model.eval()\n",
    "    test_auc = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (signal, labels) in enumerate(test_loader):\n",
    "            signal = signal.to(device); labels = labels.to(device)\n",
    "            outputs = model(signal)\n",
    "            test_auc += ml_auroc(outputs, labels.int())\n",
    "        test_auc /= len(test_loader)\n",
    "    writer.add_scalar(\"Test AUC\", test_auc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
