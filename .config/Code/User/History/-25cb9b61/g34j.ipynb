{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 22:23:55.438550: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-29 22:23:55.468429: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-29 22:23:55.468461: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-29 22:23:55.469246: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-29 22:23:55.474624: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 22:23:56.113627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch and torchvision imports\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    "import numpy as np,  matplotlib.pyplot as plt, pandas as pd\n",
    "from ResnetModel import *\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(np.transpose(np.load('X_train.npy'), axes = (0,2,1))).float()\n",
    "X_test = torch.from_numpy(np.transpose(np.load('X_test.npy'), axes = (0,2,1))).float()\n",
    "y_train = pd.read_pickle('y_train.pickle').to_numpy()\n",
    "y_test = pd.read_pickle('y_test.pickle').to_numpy()\n",
    "\n",
    "class_list = []\n",
    "for classes in y_train:\n",
    "    class_list += classes \n",
    "class_list = set(class_list)\n",
    "class_list\n",
    "diagSupclassDict = {val:i for i, val in enumerate(class_list)}\n",
    "diagSupclassDict['Nodiag'] = 5\n",
    "print(diagSupclassDict)\n",
    "\n",
    "train_label_mapping = torch.zeros((X_train.shape[0], len(diagSupclassDict)))\n",
    "print(f\"-\"*(1+30+5+35))\n",
    "for i, classes in enumerate(y_train):\n",
    "    for diagclass in classes:\n",
    "        train_label_mapping[i, diagSupclassDict[diagclass]] = 1\n",
    "    if len(classes) == 0:\n",
    "        train_label_mapping[i, diagSupclassDict['Nodiag']] = 1\n",
    "    \n",
    "    print(f\"|  {str(y_train[i]):>30}  |  {str(train_label_mapping[i]):<30}   |\")\n",
    "\n",
    "test_label_mapping = torch.zeros((X_test.shape[0], len(diagSupclassDict)))\n",
    "print(f\"-\"*(1+30+5+35))\n",
    "for i, classes in enumerate(y_test):\n",
    "    for diagclass in classes:\n",
    "        test_label_mapping[i, diagSupclassDict[diagclass]] = 1\n",
    "    if len(classes) == 0:\n",
    "        test_label_mapping[i, diagSupclassDict['Nodiag']] = 1\n",
    "    \n",
    "    print(f\"|  {str(y_train[i]):>30}  |  {str(test_label_mapping[i]):<30}   |\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, train_label_mapping)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, test_label_mapping)\n",
    "x = X_train[0:1]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=6).float()\n",
    "print(summary(model.to(device), (12,1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(4*1024-107.88)/107.88 #Max batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((model(X_train[0:10].to(device))).float(), train_label_mapping[0:10].to(device).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test AUC metric\"\"\"\n",
    "ml_auroc = MultilabelAUROC(num_labels=len(diagSupclassDict), average=\"macro\", thresholds=None)\n",
    "ml_auroc(model(X_train[0:10].to(device)), train_label_mapping[0:10].to(device).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "epochs = 10\n",
    "model = model.to(device)\n",
    "lr = 1e-6\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "lrs = []\n",
    "\n",
    "for i, (signal, labels) in enumerate(train_loader):\n",
    "    signal = signal.to(device)\n",
    "    labels = labels.to(device)\n",
    "    output = model(signal)\n",
    "    loss = criterion(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    train_loss.append(loss.item())\n",
    "    lrs.append(lr)\n",
    "    lr *= 1.1\n",
    "\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr \n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if i > 100 or lr > 1:\n",
    "        break\n",
    "\n",
    "lrs = np.array(lrs)\n",
    "train_loss = np.array(train_loss)\n",
    "\n",
    "lr_max = lrs[np.where(train_loss == train_loss.min())[0]]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(lrs, train_loss)\n",
    "plt.plot(lr_max, train_loss[lrs == lr_max], '.r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max /= 10\n",
    "lr = lr_max\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "steps_per_epoch = len(train_loader)\n",
    "T_max = steps_per_epoch*epochs\n",
    "T_0 = T_max/5 \n",
    "learning_rates = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (signal, labels) in enumerate(train_loader):\n",
    "        signal = signal.to(device); labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signal)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        if t <= T_0:\n",
    "            lr = 10**(-4) + (t/T_0)*lr_max  \n",
    "        else: \n",
    "            lr = lr_max*torch.cos((torch.pi/2)*((t-T_0)/(T_max-T_0))) + 10**(-6) \n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr \n",
    "        learning_rates.append(lr)\n",
    "        optimizer.step()\n",
    "        t+=1\n",
    "        \n",
    "        # train_AUC = ml_auroc(outputs, labels.int())\n",
    "        writer.add_scalar(\"Train_Loss\", loss, t)\n",
    "        writer.add_scalar(\"Learning rate\", lr, t)\n",
    "        # writer.add_scalar(\"Batch Train AUC\", train_AUC, t)\n",
    "\n",
    "    # model.eval()\n",
    "    # test_auc = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for i, (signal, labels) in enumerate(test_loader):\n",
    "    #         signal = signal.to(device); labels = labels.to(device)\n",
    "    #         outputs = model(signal)\n",
    "    #         test_auc += ml_auroc(outputs, labels.int())\n",
    "    #     test_auc /= len(test_loader)\n",
    "    # writer.add_scalar(\"Test AUC\", test_auc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
