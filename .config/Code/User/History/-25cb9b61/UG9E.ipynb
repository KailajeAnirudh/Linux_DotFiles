{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 20:48:26.583670: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-29 20:48:26.772323: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-29 20:48:26.772419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-29 20:48:26.797154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-29 20:48:26.855039: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 20:48:27.918093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch and torchvision imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ResnetModel import *\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(np.transpose(np.load('X_train.npy'), axes = (0,2,1))).float()\n",
    "X_test = torch.from_numpy(np.transpose(np.load('X_test.npy'), axes = (0,2,1))).float()\n",
    "y_train = pd.read_pickle('y_train.pickle').to_numpy()\n",
    "y_test = pd.read_pickle('y_test.pickle').to_numpy()\n",
    "\n",
    "class_list = []\n",
    "for classes in y_train:\n",
    "    class_list += classes \n",
    "class_list = set(class_list)\n",
    "class_list\n",
    "diagSupclassDict = {val:i for i, val in enumerate(class_list)}\n",
    "diagSupclassDict['Nodiag'] = 5\n",
    "print(diagSupclassDict)\n",
    "\n",
    "train_label_mapping = torch.zeros((X_train.shape[0], len(diagSupclassDict)))\n",
    "print(f\"-\"*(1+30+5+35))\n",
    "for i, classes in enumerate(y_train):\n",
    "    for diagclass in classes:\n",
    "        train_label_mapping[i, diagSupclassDict[diagclass]] = 1\n",
    "    if len(classes) == 0:\n",
    "        train_label_mapping[i, diagSupclassDict['Nodiag']] = 1\n",
    "    \n",
    "    print(f\"|  {str(y_train[i]):>30}  |  {str(train_label_mapping[i]):<30}   |\")\n",
    "\n",
    "test_label_mapping = torch.zeros((X_test.shape[0], len(diagSupclassDict)))\n",
    "print(f\"-\"*(1+30+5+35))\n",
    "for i, classes in enumerate(y_test):\n",
    "    for diagclass in classes:\n",
    "        test_label_mapping[i, diagSupclassDict[diagclass]] = 1\n",
    "    if len(classes) == 0:\n",
    "        test_label_mapping[i, diagSupclassDict['Nodiag']] = 1\n",
    "    \n",
    "    print(f\"|  {str(y_train[i]):>30}  |  {str(test_label_mapping[i]):<30}   |\")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, train_label_mapping)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, test_label_mapping)\n",
    "x = X_train[0:1]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=6).float()\n",
    "# print(summary(model, (12,1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 64, 500]           5,376\n",
      "              ReLU-2              [-1, 64, 500]               0\n",
      "       BatchNorm1d-3              [-1, 64, 500]             128\n",
      "         MaxPool1d-4              [-1, 64, 250]               0\n",
      "            Conv1d-5              [-1, 64, 250]           4,096\n",
      "              ReLU-6              [-1, 64, 250]               0\n",
      "       BatchNorm1d-7              [-1, 64, 250]             128\n",
      "            Conv1d-8              [-1, 64, 250]          12,288\n",
      "              ReLU-9              [-1, 64, 250]               0\n",
      "      BatchNorm1d-10              [-1, 64, 250]             128\n",
      "           Conv1d-11             [-1, 256, 250]          16,384\n",
      "      BatchNorm1d-12             [-1, 256, 250]             512\n",
      "           Conv1d-13             [-1, 256, 250]          16,384\n",
      "      BatchNorm1d-14             [-1, 256, 250]             512\n",
      "             ReLU-15             [-1, 256, 250]               0\n",
      "       Bottleneck-16             [-1, 256, 250]               0\n",
      "           Conv1d-17              [-1, 64, 250]          16,384\n",
      "             ReLU-18              [-1, 64, 250]               0\n",
      "      BatchNorm1d-19              [-1, 64, 250]             128\n",
      "           Conv1d-20              [-1, 64, 250]          12,288\n",
      "             ReLU-21              [-1, 64, 250]               0\n",
      "      BatchNorm1d-22              [-1, 64, 250]             128\n",
      "           Conv1d-23             [-1, 256, 250]          16,384\n",
      "      BatchNorm1d-24             [-1, 256, 250]             512\n",
      "             ReLU-25             [-1, 256, 250]               0\n",
      "       Bottleneck-26             [-1, 256, 250]               0\n",
      "           Conv1d-27              [-1, 64, 250]          16,384\n",
      "             ReLU-28              [-1, 64, 250]               0\n",
      "      BatchNorm1d-29              [-1, 64, 250]             128\n",
      "           Conv1d-30              [-1, 64, 250]          12,288\n",
      "             ReLU-31              [-1, 64, 250]               0\n",
      "      BatchNorm1d-32              [-1, 64, 250]             128\n",
      "           Conv1d-33             [-1, 256, 250]          16,384\n",
      "      BatchNorm1d-34             [-1, 256, 250]             512\n",
      "             ReLU-35             [-1, 256, 250]               0\n",
      "       Bottleneck-36             [-1, 256, 250]               0\n",
      "           Conv1d-37             [-1, 128, 250]          32,768\n",
      "             ReLU-38             [-1, 128, 250]               0\n",
      "      BatchNorm1d-39             [-1, 128, 250]             256\n",
      "           Conv1d-40             [-1, 128, 125]          49,152\n",
      "             ReLU-41             [-1, 128, 125]               0\n",
      "      BatchNorm1d-42             [-1, 128, 125]             256\n",
      "           Conv1d-43             [-1, 512, 125]          65,536\n",
      "      BatchNorm1d-44             [-1, 512, 125]           1,024\n",
      "           Conv1d-45             [-1, 512, 125]         131,072\n",
      "      BatchNorm1d-46             [-1, 512, 125]           1,024\n",
      "             ReLU-47             [-1, 512, 125]               0\n",
      "       Bottleneck-48             [-1, 512, 125]               0\n",
      "           Conv1d-49             [-1, 128, 125]          65,536\n",
      "             ReLU-50             [-1, 128, 125]               0\n",
      "      BatchNorm1d-51             [-1, 128, 125]             256\n",
      "           Conv1d-52             [-1, 128, 125]          49,152\n",
      "             ReLU-53             [-1, 128, 125]               0\n",
      "      BatchNorm1d-54             [-1, 128, 125]             256\n",
      "           Conv1d-55             [-1, 512, 125]          65,536\n",
      "      BatchNorm1d-56             [-1, 512, 125]           1,024\n",
      "             ReLU-57             [-1, 512, 125]               0\n",
      "       Bottleneck-58             [-1, 512, 125]               0\n",
      "           Conv1d-59             [-1, 128, 125]          65,536\n",
      "             ReLU-60             [-1, 128, 125]               0\n",
      "      BatchNorm1d-61             [-1, 128, 125]             256\n",
      "           Conv1d-62             [-1, 128, 125]          49,152\n",
      "             ReLU-63             [-1, 128, 125]               0\n",
      "      BatchNorm1d-64             [-1, 128, 125]             256\n",
      "           Conv1d-65             [-1, 512, 125]          65,536\n",
      "      BatchNorm1d-66             [-1, 512, 125]           1,024\n",
      "             ReLU-67             [-1, 512, 125]               0\n",
      "       Bottleneck-68             [-1, 512, 125]               0\n",
      "           Conv1d-69             [-1, 128, 125]          65,536\n",
      "             ReLU-70             [-1, 128, 125]               0\n",
      "      BatchNorm1d-71             [-1, 128, 125]             256\n",
      "           Conv1d-72             [-1, 128, 125]          49,152\n",
      "             ReLU-73             [-1, 128, 125]               0\n",
      "      BatchNorm1d-74             [-1, 128, 125]             256\n",
      "           Conv1d-75             [-1, 512, 125]          65,536\n",
      "      BatchNorm1d-76             [-1, 512, 125]           1,024\n",
      "             ReLU-77             [-1, 512, 125]               0\n",
      "       Bottleneck-78             [-1, 512, 125]               0\n",
      "           Conv1d-79             [-1, 256, 125]         131,072\n",
      "             ReLU-80             [-1, 256, 125]               0\n",
      "      BatchNorm1d-81             [-1, 256, 125]             512\n",
      "           Conv1d-82              [-1, 256, 63]         196,608\n",
      "             ReLU-83              [-1, 256, 63]               0\n",
      "      BatchNorm1d-84              [-1, 256, 63]             512\n",
      "           Conv1d-85             [-1, 1024, 63]         262,144\n",
      "      BatchNorm1d-86             [-1, 1024, 63]           2,048\n",
      "           Conv1d-87             [-1, 1024, 63]         524,288\n",
      "      BatchNorm1d-88             [-1, 1024, 63]           2,048\n",
      "             ReLU-89             [-1, 1024, 63]               0\n",
      "       Bottleneck-90             [-1, 1024, 63]               0\n",
      "           Conv1d-91              [-1, 256, 63]         262,144\n",
      "             ReLU-92              [-1, 256, 63]               0\n",
      "      BatchNorm1d-93              [-1, 256, 63]             512\n",
      "           Conv1d-94              [-1, 256, 63]         196,608\n",
      "             ReLU-95              [-1, 256, 63]               0\n",
      "      BatchNorm1d-96              [-1, 256, 63]             512\n",
      "           Conv1d-97             [-1, 1024, 63]         262,144\n",
      "      BatchNorm1d-98             [-1, 1024, 63]           2,048\n",
      "             ReLU-99             [-1, 1024, 63]               0\n",
      "      Bottleneck-100             [-1, 1024, 63]               0\n",
      "          Conv1d-101              [-1, 256, 63]         262,144\n",
      "            ReLU-102              [-1, 256, 63]               0\n",
      "     BatchNorm1d-103              [-1, 256, 63]             512\n",
      "          Conv1d-104              [-1, 256, 63]         196,608\n",
      "            ReLU-105              [-1, 256, 63]               0\n",
      "     BatchNorm1d-106              [-1, 256, 63]             512\n",
      "          Conv1d-107             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-108             [-1, 1024, 63]           2,048\n",
      "            ReLU-109             [-1, 1024, 63]               0\n",
      "      Bottleneck-110             [-1, 1024, 63]               0\n",
      "          Conv1d-111              [-1, 256, 63]         262,144\n",
      "            ReLU-112              [-1, 256, 63]               0\n",
      "     BatchNorm1d-113              [-1, 256, 63]             512\n",
      "          Conv1d-114              [-1, 256, 63]         196,608\n",
      "            ReLU-115              [-1, 256, 63]               0\n",
      "     BatchNorm1d-116              [-1, 256, 63]             512\n",
      "          Conv1d-117             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-118             [-1, 1024, 63]           2,048\n",
      "            ReLU-119             [-1, 1024, 63]               0\n",
      "      Bottleneck-120             [-1, 1024, 63]               0\n",
      "          Conv1d-121              [-1, 256, 63]         262,144\n",
      "            ReLU-122              [-1, 256, 63]               0\n",
      "     BatchNorm1d-123              [-1, 256, 63]             512\n",
      "          Conv1d-124              [-1, 256, 63]         196,608\n",
      "            ReLU-125              [-1, 256, 63]               0\n",
      "     BatchNorm1d-126              [-1, 256, 63]             512\n",
      "          Conv1d-127             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-128             [-1, 1024, 63]           2,048\n",
      "            ReLU-129             [-1, 1024, 63]               0\n",
      "      Bottleneck-130             [-1, 1024, 63]               0\n",
      "          Conv1d-131              [-1, 256, 63]         262,144\n",
      "            ReLU-132              [-1, 256, 63]               0\n",
      "     BatchNorm1d-133              [-1, 256, 63]             512\n",
      "          Conv1d-134              [-1, 256, 63]         196,608\n",
      "            ReLU-135              [-1, 256, 63]               0\n",
      "     BatchNorm1d-136              [-1, 256, 63]             512\n",
      "          Conv1d-137             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-138             [-1, 1024, 63]           2,048\n",
      "            ReLU-139             [-1, 1024, 63]               0\n",
      "      Bottleneck-140             [-1, 1024, 63]               0\n",
      "          Conv1d-141              [-1, 256, 63]         262,144\n",
      "            ReLU-142              [-1, 256, 63]               0\n",
      "     BatchNorm1d-143              [-1, 256, 63]             512\n",
      "          Conv1d-144              [-1, 256, 63]         196,608\n",
      "            ReLU-145              [-1, 256, 63]               0\n",
      "     BatchNorm1d-146              [-1, 256, 63]             512\n",
      "          Conv1d-147             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-148             [-1, 1024, 63]           2,048\n",
      "            ReLU-149             [-1, 1024, 63]               0\n",
      "      Bottleneck-150             [-1, 1024, 63]               0\n",
      "          Conv1d-151              [-1, 256, 63]         262,144\n",
      "            ReLU-152              [-1, 256, 63]               0\n",
      "     BatchNorm1d-153              [-1, 256, 63]             512\n",
      "          Conv1d-154              [-1, 256, 63]         196,608\n",
      "            ReLU-155              [-1, 256, 63]               0\n",
      "     BatchNorm1d-156              [-1, 256, 63]             512\n",
      "          Conv1d-157             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-158             [-1, 1024, 63]           2,048\n",
      "            ReLU-159             [-1, 1024, 63]               0\n",
      "      Bottleneck-160             [-1, 1024, 63]               0\n",
      "          Conv1d-161              [-1, 256, 63]         262,144\n",
      "            ReLU-162              [-1, 256, 63]               0\n",
      "     BatchNorm1d-163              [-1, 256, 63]             512\n",
      "          Conv1d-164              [-1, 256, 63]         196,608\n",
      "            ReLU-165              [-1, 256, 63]               0\n",
      "     BatchNorm1d-166              [-1, 256, 63]             512\n",
      "          Conv1d-167             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-168             [-1, 1024, 63]           2,048\n",
      "            ReLU-169             [-1, 1024, 63]               0\n",
      "      Bottleneck-170             [-1, 1024, 63]               0\n",
      "          Conv1d-171              [-1, 256, 63]         262,144\n",
      "            ReLU-172              [-1, 256, 63]               0\n",
      "     BatchNorm1d-173              [-1, 256, 63]             512\n",
      "          Conv1d-174              [-1, 256, 63]         196,608\n",
      "            ReLU-175              [-1, 256, 63]               0\n",
      "     BatchNorm1d-176              [-1, 256, 63]             512\n",
      "          Conv1d-177             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-178             [-1, 1024, 63]           2,048\n",
      "            ReLU-179             [-1, 1024, 63]               0\n",
      "      Bottleneck-180             [-1, 1024, 63]               0\n",
      "          Conv1d-181              [-1, 256, 63]         262,144\n",
      "            ReLU-182              [-1, 256, 63]               0\n",
      "     BatchNorm1d-183              [-1, 256, 63]             512\n",
      "          Conv1d-184              [-1, 256, 63]         196,608\n",
      "            ReLU-185              [-1, 256, 63]               0\n",
      "     BatchNorm1d-186              [-1, 256, 63]             512\n",
      "          Conv1d-187             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-188             [-1, 1024, 63]           2,048\n",
      "            ReLU-189             [-1, 1024, 63]               0\n",
      "      Bottleneck-190             [-1, 1024, 63]               0\n",
      "          Conv1d-191              [-1, 256, 63]         262,144\n",
      "            ReLU-192              [-1, 256, 63]               0\n",
      "     BatchNorm1d-193              [-1, 256, 63]             512\n",
      "          Conv1d-194              [-1, 256, 63]         196,608\n",
      "            ReLU-195              [-1, 256, 63]               0\n",
      "     BatchNorm1d-196              [-1, 256, 63]             512\n",
      "          Conv1d-197             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-198             [-1, 1024, 63]           2,048\n",
      "            ReLU-199             [-1, 1024, 63]               0\n",
      "      Bottleneck-200             [-1, 1024, 63]               0\n",
      "          Conv1d-201              [-1, 256, 63]         262,144\n",
      "            ReLU-202              [-1, 256, 63]               0\n",
      "     BatchNorm1d-203              [-1, 256, 63]             512\n",
      "          Conv1d-204              [-1, 256, 63]         196,608\n",
      "            ReLU-205              [-1, 256, 63]               0\n",
      "     BatchNorm1d-206              [-1, 256, 63]             512\n",
      "          Conv1d-207             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-208             [-1, 1024, 63]           2,048\n",
      "            ReLU-209             [-1, 1024, 63]               0\n",
      "      Bottleneck-210             [-1, 1024, 63]               0\n",
      "          Conv1d-211              [-1, 256, 63]         262,144\n",
      "            ReLU-212              [-1, 256, 63]               0\n",
      "     BatchNorm1d-213              [-1, 256, 63]             512\n",
      "          Conv1d-214              [-1, 256, 63]         196,608\n",
      "            ReLU-215              [-1, 256, 63]               0\n",
      "     BatchNorm1d-216              [-1, 256, 63]             512\n",
      "          Conv1d-217             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-218             [-1, 1024, 63]           2,048\n",
      "            ReLU-219             [-1, 1024, 63]               0\n",
      "      Bottleneck-220             [-1, 1024, 63]               0\n",
      "          Conv1d-221              [-1, 256, 63]         262,144\n",
      "            ReLU-222              [-1, 256, 63]               0\n",
      "     BatchNorm1d-223              [-1, 256, 63]             512\n",
      "          Conv1d-224              [-1, 256, 63]         196,608\n",
      "            ReLU-225              [-1, 256, 63]               0\n",
      "     BatchNorm1d-226              [-1, 256, 63]             512\n",
      "          Conv1d-227             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-228             [-1, 1024, 63]           2,048\n",
      "            ReLU-229             [-1, 1024, 63]               0\n",
      "      Bottleneck-230             [-1, 1024, 63]               0\n",
      "          Conv1d-231              [-1, 256, 63]         262,144\n",
      "            ReLU-232              [-1, 256, 63]               0\n",
      "     BatchNorm1d-233              [-1, 256, 63]             512\n",
      "          Conv1d-234              [-1, 256, 63]         196,608\n",
      "            ReLU-235              [-1, 256, 63]               0\n",
      "     BatchNorm1d-236              [-1, 256, 63]             512\n",
      "          Conv1d-237             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-238             [-1, 1024, 63]           2,048\n",
      "            ReLU-239             [-1, 1024, 63]               0\n",
      "      Bottleneck-240             [-1, 1024, 63]               0\n",
      "          Conv1d-241              [-1, 256, 63]         262,144\n",
      "            ReLU-242              [-1, 256, 63]               0\n",
      "     BatchNorm1d-243              [-1, 256, 63]             512\n",
      "          Conv1d-244              [-1, 256, 63]         196,608\n",
      "            ReLU-245              [-1, 256, 63]               0\n",
      "     BatchNorm1d-246              [-1, 256, 63]             512\n",
      "          Conv1d-247             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-248             [-1, 1024, 63]           2,048\n",
      "            ReLU-249             [-1, 1024, 63]               0\n",
      "      Bottleneck-250             [-1, 1024, 63]               0\n",
      "          Conv1d-251              [-1, 256, 63]         262,144\n",
      "            ReLU-252              [-1, 256, 63]               0\n",
      "     BatchNorm1d-253              [-1, 256, 63]             512\n",
      "          Conv1d-254              [-1, 256, 63]         196,608\n",
      "            ReLU-255              [-1, 256, 63]               0\n",
      "     BatchNorm1d-256              [-1, 256, 63]             512\n",
      "          Conv1d-257             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-258             [-1, 1024, 63]           2,048\n",
      "            ReLU-259             [-1, 1024, 63]               0\n",
      "      Bottleneck-260             [-1, 1024, 63]               0\n",
      "          Conv1d-261              [-1, 256, 63]         262,144\n",
      "            ReLU-262              [-1, 256, 63]               0\n",
      "     BatchNorm1d-263              [-1, 256, 63]             512\n",
      "          Conv1d-264              [-1, 256, 63]         196,608\n",
      "            ReLU-265              [-1, 256, 63]               0\n",
      "     BatchNorm1d-266              [-1, 256, 63]             512\n",
      "          Conv1d-267             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-268             [-1, 1024, 63]           2,048\n",
      "            ReLU-269             [-1, 1024, 63]               0\n",
      "      Bottleneck-270             [-1, 1024, 63]               0\n",
      "          Conv1d-271              [-1, 256, 63]         262,144\n",
      "            ReLU-272              [-1, 256, 63]               0\n",
      "     BatchNorm1d-273              [-1, 256, 63]             512\n",
      "          Conv1d-274              [-1, 256, 63]         196,608\n",
      "            ReLU-275              [-1, 256, 63]               0\n",
      "     BatchNorm1d-276              [-1, 256, 63]             512\n",
      "          Conv1d-277             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-278             [-1, 1024, 63]           2,048\n",
      "            ReLU-279             [-1, 1024, 63]               0\n",
      "      Bottleneck-280             [-1, 1024, 63]               0\n",
      "          Conv1d-281              [-1, 256, 63]         262,144\n",
      "            ReLU-282              [-1, 256, 63]               0\n",
      "     BatchNorm1d-283              [-1, 256, 63]             512\n",
      "          Conv1d-284              [-1, 256, 63]         196,608\n",
      "            ReLU-285              [-1, 256, 63]               0\n",
      "     BatchNorm1d-286              [-1, 256, 63]             512\n",
      "          Conv1d-287             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-288             [-1, 1024, 63]           2,048\n",
      "            ReLU-289             [-1, 1024, 63]               0\n",
      "      Bottleneck-290             [-1, 1024, 63]               0\n",
      "          Conv1d-291              [-1, 256, 63]         262,144\n",
      "            ReLU-292              [-1, 256, 63]               0\n",
      "     BatchNorm1d-293              [-1, 256, 63]             512\n",
      "          Conv1d-294              [-1, 256, 63]         196,608\n",
      "            ReLU-295              [-1, 256, 63]               0\n",
      "     BatchNorm1d-296              [-1, 256, 63]             512\n",
      "          Conv1d-297             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-298             [-1, 1024, 63]           2,048\n",
      "            ReLU-299             [-1, 1024, 63]               0\n",
      "      Bottleneck-300             [-1, 1024, 63]               0\n",
      "          Conv1d-301              [-1, 256, 63]         262,144\n",
      "            ReLU-302              [-1, 256, 63]               0\n",
      "     BatchNorm1d-303              [-1, 256, 63]             512\n",
      "          Conv1d-304              [-1, 256, 63]         196,608\n",
      "            ReLU-305              [-1, 256, 63]               0\n",
      "     BatchNorm1d-306              [-1, 256, 63]             512\n",
      "          Conv1d-307             [-1, 1024, 63]         262,144\n",
      "     BatchNorm1d-308             [-1, 1024, 63]           2,048\n",
      "            ReLU-309             [-1, 1024, 63]               0\n",
      "      Bottleneck-310             [-1, 1024, 63]               0\n",
      "          Conv1d-311              [-1, 512, 63]         524,288\n",
      "            ReLU-312              [-1, 512, 63]               0\n",
      "     BatchNorm1d-313              [-1, 512, 63]           1,024\n",
      "          Conv1d-314              [-1, 512, 32]         786,432\n",
      "            ReLU-315              [-1, 512, 32]               0\n",
      "     BatchNorm1d-316              [-1, 512, 32]           1,024\n",
      "          Conv1d-317             [-1, 2048, 32]       1,048,576\n",
      "     BatchNorm1d-318             [-1, 2048, 32]           4,096\n",
      "          Conv1d-319             [-1, 2048, 32]       2,097,152\n",
      "     BatchNorm1d-320             [-1, 2048, 32]           4,096\n",
      "            ReLU-321             [-1, 2048, 32]               0\n",
      "      Bottleneck-322             [-1, 2048, 32]               0\n",
      "          Conv1d-323              [-1, 512, 32]       1,048,576\n",
      "            ReLU-324              [-1, 512, 32]               0\n",
      "     BatchNorm1d-325              [-1, 512, 32]           1,024\n",
      "          Conv1d-326              [-1, 512, 32]         786,432\n",
      "            ReLU-327              [-1, 512, 32]               0\n",
      "     BatchNorm1d-328              [-1, 512, 32]           1,024\n",
      "          Conv1d-329             [-1, 2048, 32]       1,048,576\n",
      "     BatchNorm1d-330             [-1, 2048, 32]           4,096\n",
      "            ReLU-331             [-1, 2048, 32]               0\n",
      "      Bottleneck-332             [-1, 2048, 32]               0\n",
      "          Conv1d-333              [-1, 512, 32]       1,048,576\n",
      "            ReLU-334              [-1, 512, 32]               0\n",
      "     BatchNorm1d-335              [-1, 512, 32]           1,024\n",
      "          Conv1d-336              [-1, 512, 32]         786,432\n",
      "            ReLU-337              [-1, 512, 32]               0\n",
      "     BatchNorm1d-338              [-1, 512, 32]           1,024\n",
      "          Conv1d-339             [-1, 2048, 32]       1,048,576\n",
      "     BatchNorm1d-340             [-1, 2048, 32]           4,096\n",
      "            ReLU-341             [-1, 2048, 32]               0\n",
      "      Bottleneck-342             [-1, 2048, 32]               0\n",
      "AdaptiveAvgPool1d-343              [-1, 2048, 1]               0\n",
      "          Linear-344                    [-1, 6]          12,294\n",
      "            ReLU-345                    [-1, 6]               0\n",
      "================================================================\n",
      "Total params: 28,278,918\n",
      "Trainable params: 28,278,918\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 95.21\n",
      "Params size (MB): 107.88\n",
      "Estimated Total Size (MB): 203.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.to(device), (12,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.96811271783463"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4*1024-107.88)/107.88 #Max batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "epochs = 10\n",
    "model = model.to(device)\n",
    "lr = 1e-6\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "lrs = []\n",
    "\n",
    "for i, (signal, labels) in enumerate(train_loader):\n",
    "    signal = signal.to(device)\n",
    "    labels = labels.to(device)\n",
    "    output = model(signal)\n",
    "    loss = criterion(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    train_loss.append(loss.item())\n",
    "    lrs.append(lr)\n",
    "    lr *= 1.1\n",
    "\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr \n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if i > 100 or lr > 1:\n",
    "        break\n",
    "\n",
    "lrs = np.array(lrs)\n",
    "train_loss = np.array(train_loss)\n",
    "\n",
    "lr_max = lrs[np.where(train_loss == train_loss.min())[0]]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(lrs, train_loss)\n",
    "plt.plot(lr_max, train_loss[lrs == lr_max], '.r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.11137767e-05])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max /= 10\n",
    "lr = lr_max\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "steps_per_epoch = len(train_loader)\n",
    "T_max = steps_per_epoch*epochs\n",
    "T_0 = T_max/5 \n",
    "learning_rates = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (signal, labels) in enumerate(train_loader):\n",
    "        signal = signal.to(device); labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signal)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        if t <= T_0:\n",
    "            lr = 10**(-4) + (t/T_0)*lr_max  \n",
    "        else: \n",
    "            lr = lr_max*np.cos((np.pi/2)*((t-T_0)/(T_max-T_0))) + 10**(-6) \n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr \n",
    "        learning_rates.append(lr)\n",
    "        optimizer.step()\n",
    "        t+=1\n",
    "        writer.add_scalar(\"Train_Loss\", loss, t)\n",
    "        writer.add_scalar(\"Learning rate\", lr, t)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
