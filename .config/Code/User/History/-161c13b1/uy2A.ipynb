{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a9LJFb4k6bEt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-14 20:37:30.780093: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-11-14 20:37:30.818116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:37:30.818151: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:37:30.819149: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:37:30.825151: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 20:37:31.385854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "import tensorflow \n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPkwW2yfFGh7"
      },
      "source": [
        "(a) We will use the MNIST dataset for tranining and testing here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAvtHoCT8VhG",
        "outputId": "9542aa67-3394-4c6f-c577-4f27765c84c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1025070/321430160.py:52: RuntimeWarning: invalid value encountered in divide\n",
            "  x_tr = np.nan_to_num(np.array(x_tr)/np.sum(x_tr,axis=0))\n",
            "/tmp/ipykernel_1025070/321430160.py:53: RuntimeWarning: invalid value encountered in divide\n",
            "  x_te = np.nan_to_num(np.array(x_te)/np.sum(x_tr,axis=0))\n"
          ]
        }
      ],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() \n",
        "train_idx_0 = np.argwhere(Y_train==0)\n",
        "train_idx_1 = np.argwhere(Y_train==1)\n",
        "test_idx_0 = np.argwhere(Y_test==0)\n",
        "test_idx_1 = np.argwhere(Y_test==1)\n",
        "x_train = [] \n",
        "y_train = [] \n",
        "x_test = [] \n",
        "y_test = [] \n",
        "for i in range(len(train_idx_0)):\n",
        "  idx = train_idx_0[i][0]\n",
        "  temp = cv2.resize(X_train[idx], (14,14))\n",
        "  x_train.append(temp.flatten())\n",
        "  y_train.append(Y_train[idx])\n",
        "\n",
        "for i in range(len(train_idx_1)):\n",
        "  idx = train_idx_1[i][0]\n",
        "  temp = cv2.resize(X_train[idx], (14,14))\n",
        "  x_train.append(temp.flatten())\n",
        "  y_train.append(Y_train[idx])\n",
        "\n",
        "for i in range(len(test_idx_0)):\n",
        "  idx = test_idx_0[i][0]\n",
        "  temp = cv2.resize(X_test[idx], (14,14))\n",
        "  x_test.append(temp.flatten())\n",
        "  y_test.append(Y_test[idx])\n",
        "\n",
        "for i in range(len(test_idx_1)):\n",
        "  idx = test_idx_1[i][0]\n",
        "  temp = cv2.resize(X_test[idx], (14,14))\n",
        "  x_test.append(temp.flatten())\n",
        "  y_test.append(Y_test[idx])\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "idx_train=np.array(range(len(y_train)))\n",
        "idx_test=np.array(range(len(y_test)))\n",
        "np.random.shuffle(idx_train)\n",
        "np.random.shuffle(idx_test)\n",
        "x_tr = []\n",
        "x_te = []\n",
        "y_tr = []\n",
        "y_te = []\n",
        "for i in range(len(idx_train)):\n",
        "  x_tr.append(x_train[idx_train[i]].flatten())\n",
        "  y_tr.append(y_train[idx_train[i]])\n",
        "for i in range(len(idx_test)):\n",
        "  x_te.append(x_test[idx_test[i]].flatten())\n",
        "  y_te.append(y_test[idx_test[i]])\n",
        "x_tr = np.nan_to_num(np.array(x_tr)/np.sum(x_tr,axis=0)) \n",
        "x_te = np.nan_to_num(np.array(x_te)/np.sum(x_tr,axis=0)) \n",
        "y_tr = np.array(y_tr)\n",
        "y_te = np.array(y_te)\n",
        "y_train = [] \n",
        "y_test = [] \n",
        "for i in range(len(y_tr)): \n",
        "  if y_tr[i] == 1:\n",
        "    y_train.append(-1)\n",
        "  else:\n",
        "     y_train.append(1)\n",
        "\n",
        "for i in range(len(y_te)): \n",
        "  if y_te[i] == 1:\n",
        "    y_test.append(-1)\n",
        "  else:\n",
        "     y_test.append(1)\n",
        "y_tr = np.array(y_train)\n",
        "y_te = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9juecrvjvwo"
      },
      "source": [
        "(b) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nSpufeIx04Q"
      },
      "source": [
        "Funtion grad_h implements the gradient for logistic regression. Function train_gd trains the model with a gradient descent algorithm. Function loss computes the logistic loss. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fgHCDBTYjw81"
      },
      "outputs": [],
      "source": [
        "def grad_h(w,x,y,lam,w0): \n",
        "  ##Paramaters: \n",
        "  # w: weight matrix (cxd array)\n",
        "  # x: data matrix (nxd array)\n",
        "  # y: true label array (nx1 array)\n",
        "  # lam: regularization constant (constant)\n",
        "  # w0: bias matrix (cxn array)\n",
        "  #Output: grad (cxd array)\n",
        "  n = np.shape(x)[0]\n",
        "  grad = -(1/n)*np.matmul(x.T,((y*(np.exp(-y*(np.matmul(w,x.T)+w0)))).T) /(np.exp(-y*(np.matmul(w,x.T)+w0))+1).T)+lam*w.T\n",
        "  return grad.T\n",
        "\n",
        "def get_grad(w, lambda_i, X, Y, w0 = 0):\n",
        "    print(w.shape, X.shape, Y.shape, w0)\n",
        "    sigmoid_arg = np.exp(-Y*(X@w + w0))\n",
        "    sigmoid = sigmoid_arg/(1+sigmoid_arg)\n",
        "    grad = -(1/X.shape[0])* sigmoid@(Y[:, np.newaxis]*X)+ lambda_i*w\n",
        "    return grad\n",
        "\n",
        "def get_loss(w, lambda_i, X, Y, w0 = 0):\n",
        "    loss = (1/X.shape[0])*(np.log(1+np.exp(-Y*(X@w + w0)))).sum(axis = 0) + (lambda_i/2)*(np.linalg.norm(w)**2 + w0**2)\n",
        "    return loss \n",
        "\n",
        "def train_gd(x_tr, y_tr, lam, lr, n_iter, w0=0,w_0 = None):\n",
        "  ##Paramaters: \n",
        "  # x_tr: data matrix (nxd array)\n",
        "  # y_tr: true label array (nx1 array)\n",
        "  # lam: regularization constant (constant)\n",
        "  # lr: learning rate \n",
        "  # n_iter: number of iteration steps \n",
        "  # w0: bias matrix (cxn array)\n",
        "  #Output: grad (cxd array)\n",
        "  c = 2   \n",
        "  d = np.shape(x_tr)[1]\n",
        "  w = w_0 or np.random.rand(c,d)\n",
        "  w_all = []\n",
        "  t = 0 \n",
        "  while t <= n_iter: \n",
        "    # print(w.shape)\n",
        "    # if (np.all(np.abs(grad_h(w, x_tr, y_tr, lam, w0)[0] - get_grad(w[0], lam, x_tr, y_tr, 0)) < 1e-3)) == False:\n",
        "    #    print(\"False\")\n",
        "    \n",
        "    w = w - lr*grad_h(w,x_tr,y_tr,lam,w0)\n",
        "    w_all.append(w)\n",
        "    t += 1 \n",
        "  return np.array(w_all) \n",
        "\n",
        "def train_gd2(x_tr, y_tr, lam, lr, n_iter, w0=0,w_0 = None):\n",
        "  ##Paramaters: \n",
        "  # x_tr: data matrix (nxd array)\n",
        "  # y_tr: true label array (nx1 array)\n",
        "  # lam: regularization constant (constant)\n",
        "  # lr: learning rate \n",
        "  # n_iter: number of iteration steps \n",
        "  # w0: bias matrix (cxn array)\n",
        "  #Output: grad (cxd array)\n",
        "  c = 2   \n",
        "  d = np.shape(x_tr)[1]\n",
        "  wt = np.random.standard_normal((28*28))\n",
        "  w0 = 0\n",
        "  w_all = []\n",
        "  t = 0 \n",
        "  while t <= n_iter: \n",
        "    gradw, gradw0 = get_grad(w=wt, X= x_tr, Y= y_tr, lambda_i=lam, w0 = w0)\n",
        "    wt = wt - lr*gradw\n",
        "    w0 = w0 - lr*gradw0\n",
        "    w_all.append((wt, w0))\n",
        "    t += 1 \n",
        "  return np.array(w_all) \n",
        "\n",
        "def loss(w, x, y, lam, w0=0):\n",
        "  ##Paramaters: \n",
        "  # x_tr: data matrix (nxd array)\n",
        "  # y_tr: true label array (nx1 array)\n",
        "  # lam: regularization constant (constant)\n",
        "  # lr: learning rate \n",
        "  # n_iter: number of iteration steps \n",
        "  # w0: bias matrix (cxn array)\n",
        "  #Output: grad (cxd array)\n",
        "  l1 = np.sum(np.log(1 + np.exp(-y*(np.matmul(w,x.T)+w0))+1),axis=1)/np.shape(y_tr)[0] \n",
        "  l2 = (lam/2)*((np.linalg.norm(w)*np.linalg.norm(w)) + (np.linalg.norm(w0)*np.linalg.norm(w0)))\n",
        "  return l1 + l2\n",
        "\n",
        "def logit(w, x, w0=0):\n",
        "  ##Paramaters: \n",
        "  # x_tr: data matrix (nxd array)\n",
        "  # y_tr: true label array (nx1 array)\n",
        "  # lam: regularization constant (constant)\n",
        "  # lr: learning rate \n",
        "  # n_iter: number of iteration steps \n",
        "  # w0: bias matrix (cxn array)\n",
        "  #Output: grad (cxd array)\n",
        "  y_0 = 1 \n",
        "  y_1 = -1 \n",
        "  p = lambda x, y, w, w0: (np.exp(-y*(np.matmul(w,x.T)+w0))+1)**(-1)\n",
        "  prob = np.array([p(x,y_0,w,w0),p(x,y_1,w,w0)]) \n",
        "  idx = np.argmax(prob)\n",
        "  return idx \n",
        "\n",
        "def get_grad(w, lambda_i, X, Y, w0 = 0):\n",
        "    sigmoid_arg = np.exp(-Y*(X@w + w0))\n",
        "    sigmoid = sigmoid_arg/(1+sigmoid_arg)\n",
        "    grad = -(1/X.shape[0])* sigmoid@(Y[:, np.newaxis]*X)+ lambda_i*w\n",
        "    gradw0 = -Y*sigmoid*(1/X.shape[0]) + lambda_i*w0\n",
        "    return grad, gradw0\n",
        "\n",
        "def get_loss(w, lambda_i, X, Y, w0 = 0):\n",
        "    loss = (1/X.shape[0])*(np.log(1+np.exp(-Y*(X@w + w0)))).sum(axis = 0) + (lambda_i/2)*(np.linalg.norm(w)**2 + w0**2)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVWrLbt2yM9b"
      },
      "source": [
        "Trained weights now produced. Regularization constant varied, learning rate set to 0.01, trained over 50 time-steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kHFuO5rqSggy"
      },
      "outputs": [],
      "source": [
        "lambdas = [1,10,20,30,40,50,70] ##regularization parameter \n",
        "w_d = 0.01 ##learning rate \n",
        "T_max = 50  #max iterations \n",
        "w_all_1 = train_gd(x_tr, y_tr, lambdas[0], w_d, T_max)\n",
        "w_all_2 = train_gd(x_tr, y_tr, lambdas[1], w_d, T_max)\n",
        "w_all_3 = train_gd(x_tr, y_tr, lambdas[2], w_d, T_max)\n",
        "w_all_4 = train_gd(x_tr, y_tr, lambdas[3], w_d, T_max)\n",
        "w_all_5 = train_gd(x_tr, y_tr, lambdas[4], w_d, T_max)\n",
        "w_all_6 = train_gd(x_tr, y_tr, lambdas[5], w_d, T_max)\n",
        "w_all_7 = train_gd(x_tr, y_tr, lambdas[6], w_d, T_max)\n",
        "loss_tr_1 = [loss(w,x_tr, y_tr, lambdas[0])[0] for w in w_all_1]\n",
        "loss_tr_2 = [loss(w,x_tr, y_tr, lambdas[1])[0] for w in w_all_2]\n",
        "loss_tr_3 = [loss(w,x_tr, y_tr, lambdas[2])[0] for w in w_all_3]\n",
        "loss_tr_4 = [loss(w,x_tr, y_tr, lambdas[3])[0] for w in w_all_4]\n",
        "loss_tr_5 = [loss(w,x_tr, y_tr, lambdas[4])[0] for w in w_all_5]\n",
        "loss_tr_6 = [loss(w,x_tr, y_tr, lambdas[5])[0] for w in w_all_6]\n",
        "loss_tr_7 = [loss(w,x_tr, y_tr, lambdas[6])[0] for w in w_all_7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 784 is different from 196)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m w_all \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m t \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m gradw, gradw0 \u001b[39m=\u001b[39m get_grad(w\u001b[39m=\u001b[39;49mwt, X\u001b[39m=\u001b[39;49m x_tr, Y\u001b[39m=\u001b[39;49m y_tr, lambda_i\u001b[39m=\u001b[39;49mlambdas[\u001b[39m0\u001b[39;49m], w0 \u001b[39m=\u001b[39;49m w0)\n",
            "\u001b[1;32m/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X46sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_grad\u001b[39m(w, lambda_i, X, Y, w0 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X46sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     sigmoid_arg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mY\u001b[39m*\u001b[39m(X\u001b[39m@w\u001b[39;49m \u001b[39m+\u001b[39m w0))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X46sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     sigmoid \u001b[39m=\u001b[39m sigmoid_arg\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39msigmoid_arg)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X46sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     grad \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\u001b[39m*\u001b[39m sigmoid\u001b[39m@\u001b[39m(Y[:, np\u001b[39m.\u001b[39mnewaxis]\u001b[39m*\u001b[39mX)\u001b[39m+\u001b[39m lambda_i\u001b[39m*\u001b[39mw\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 784 is different from 196)"
          ]
        }
      ],
      "source": [
        "wt = np.random.standard_normal((28*28))\n",
        "w0 = 0\n",
        "w_all = []\n",
        "t = 0 \n",
        "\n",
        "gradw, gradw0 = get_grad(w=wt, X= x_tr, Y= y_tr, lambda_i=lambdas[0], w0 = w0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 784 is different from 196)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m w_all_1_a \u001b[39m=\u001b[39m train_gd2(x_tr, y_tr, lambdas[\u001b[39m0\u001b[39;49m], w_d, T_max)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m w_all_2_a \u001b[39m=\u001b[39m train_gd2(x_tr, y_tr, lambdas[\u001b[39m1\u001b[39m], w_d, T_max)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m w_all_3_a \u001b[39m=\u001b[39m train_gd2(x_tr, y_tr, lambdas[\u001b[39m2\u001b[39m], w_d, T_max)\n",
            "\u001b[1;32m/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m t \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mwhile\u001b[39;00m t \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_iter: \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m   gradw, gradw0 \u001b[39m=\u001b[39m get_grad(w\u001b[39m=\u001b[39;49mwt, X\u001b[39m=\u001b[39;49m x_tr, Y\u001b[39m=\u001b[39;49m y_tr, lambda_i\u001b[39m=\u001b[39;49mlam, w0 \u001b[39m=\u001b[39;49m w0)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m   wt \u001b[39m=\u001b[39m wt \u001b[39m-\u001b[39m lr\u001b[39m*\u001b[39mgradw\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m   w0 \u001b[39m=\u001b[39m w0 \u001b[39m-\u001b[39m lr\u001b[39m*\u001b[39mgradw0\n",
            "\u001b[1;32m/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_grad\u001b[39m(w, lambda_i, X, Y, w0 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     sigmoid_arg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mY\u001b[39m*\u001b[39m(X\u001b[39m@w\u001b[39;49m \u001b[39m+\u001b[39m w0))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     sigmoid \u001b[39m=\u001b[39m sigmoid_arg\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39msigmoid_arg)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/anirudhkailaje/Documents/01_UPenn/01_ESE5460/02_Homework/04_Homework4/leclerc_nima_hw4_prob1.ipynb#X44sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     grad \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\u001b[39m*\u001b[39m sigmoid\u001b[39m@\u001b[39m(Y[:, np\u001b[39m.\u001b[39mnewaxis]\u001b[39m*\u001b[39mX)\u001b[39m+\u001b[39m lambda_i\u001b[39m*\u001b[39mw\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 784 is different from 196)"
          ]
        }
      ],
      "source": [
        "w_all_1_a = train_gd2(x_tr, y_tr, lambdas[0], w_d, T_max)\n",
        "w_all_2_a = train_gd2(x_tr, y_tr, lambdas[1], w_d, T_max)\n",
        "w_all_3_a = train_gd2(x_tr, y_tr, lambdas[2], w_d, T_max)\n",
        "w_all_4_a = train_gd2(x_tr, y_tr, lambdas[3], w_d, T_max)\n",
        "w_all_5_a = train_gd2(x_tr, y_tr, lambdas[4], w_d, T_max)\n",
        "w_all_6_a = train_gd2(x_tr, y_tr, lambdas[5], w_d, T_max)\n",
        "w_all_7_a = train_gd2(x_tr, y_tr, lambdas[6], w_d, T_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_tr_1_a = [get_loss(w,x_tr, y_tr, lambdas[0])[0] for w in w_all_1_a]\n",
        "loss_tr_2_a = [get_loss(w,x_tr, y_tr, lambdas[1])[0] for w in w_all_2_a]\n",
        "loss_tr_3_a = [get_loss(w,x_tr, y_tr, lambdas[2])[0] for w in w_all_3_a]\n",
        "loss_tr_4_a = [get_loss(w,x_tr, y_tr, lambdas[3])[0] for w in w_all_4_a]\n",
        "loss_tr_5_a = [get_loss(w,x_tr, y_tr, lambdas[4])[0] for w in w_all_5_a]\n",
        "loss_tr_6_a = [get_loss(w,x_tr, y_tr, lambdas[5])[0] for w in w_all_6_a]\n",
        "loss_tr_7_a = [get_loss(w,x_tr, y_tr, lambdas[6])[0] for w in w_all_7_a]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-ouMLljeeFD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ko3FoRhya9h"
      },
      "source": [
        "Gradient descent training loss plotted over iteration number in semi-log scale. Shows initial linear drop, prior to convergence. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "jBdQQlzAdYS4",
        "outputId": "22f43b8b-96da-4dfa-d2c3-f49f3f52c0a4"
      },
      "outputs": [],
      "source": [
        "t_rng = np.array(range(len(w_all_1))) \n",
        "plot.semilogy(t_rng, loss_tr_1)\n",
        "plot.semilogy(t_rng, loss_tr_2)\n",
        "plot.semilogy(t_rng, loss_tr_3)\n",
        "plot.semilogy(t_rng, loss_tr_4)\n",
        "plot.semilogy(t_rng, loss_tr_5)\n",
        "plot.semilogy(t_rng, loss_tr_6)\n",
        "plot.semilogy(t_rng, loss_tr_7)\n",
        "plot.legend(('$\\lambda = 1$','$\\lambda = 10$','$\\lambda = 20$','$\\lambda = 30$','$\\lambda = 40$','$\\lambda = 50$','$\\lambda = 70$') )\n",
        "plot.ylabel('Training loss [log-scale]')\n",
        "plot.xlabel('Iteration number')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY2AXklxgTkN"
      },
      "source": [
        "From this, we can extract the slope for each loss function as done below (in log-scale).  \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0734DLM9gpEN"
      },
      "outputs": [],
      "source": [
        "fit_1 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_1[0:20]), 1)\n",
        "fit_2 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_2[0:20]), 1)\n",
        "fit_3 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_3[0:20]), 1)\n",
        "fit_4 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_4[0:20]), 1)\n",
        "fit_5 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_5[0:20]), 1)\n",
        "fit_6 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_6[0:20]), 1)\n",
        "fit_7 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_7[0:20]), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-4Pqq-baS0h",
        "outputId": "28d8d0f2-fd44-4f6e-bb0f-34663a77df3d"
      },
      "outputs": [],
      "source": [
        "print(\"Lambda = 1:\"+\" \"+ str(fit_1[0]))\n",
        "print(\"Lambda = 10:\"+\" \"+ str(fit_2[0]))\n",
        "print(\"Lambda = 20:\"+\" \"+ str(fit_3[0]))\n",
        "print(\"Lambda = 30:\"+\" \"+ str(fit_4[0]))\n",
        "print(\"Lambda = 40:\"+\" \"+ str(fit_5[0]))\n",
        "print(\"Lambda = 50:\"+\" \"+ str(fit_6[0]))\n",
        "print(\"Lambda = 70:\"+\" \"+ str(fit_7[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgqI1u61yrNT"
      },
      "source": [
        "We find that that the optimal $\\lambda$ is $\\lambda = 10$. The loss function for larger $\\lambda$ begins to saturate. After $\\lambda = 50$, bias dominates due to regularization.  Hence, we will take $\\lambda = 10$ with a slope of -0.32, corresponding to a condition nubmer $\\kappa = 3.12$.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076pM_xHy7fo"
      },
      "source": [
        "(d) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXswSJxMka9a"
      },
      "source": [
        "The model is now trained using Nesterov's momentum. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccmuC7CBy7RB"
      },
      "outputs": [],
      "source": [
        "def train_nest(x_tr, y_tr, rho, lam, lr, n_iter, w0=0,w_0 = None):  \n",
        "  ##Paramaters: \n",
        "  # x_tr: data matrix (nxd array)\n",
        "  # y_tr: true label array (nx1 array)\n",
        "  # rho: momentum parameter \n",
        "  # lam: regularization constant (constant)\n",
        "  # lr: learning rate \n",
        "  # n_iter: number of iteration steps \n",
        "  # w0: bias matrix (cxn array)\n",
        "  #Output: grad (cxd array)\n",
        "  \n",
        "  w_all = []\n",
        "  t = 0 \n",
        "  c = 2  \n",
        "  d = np.shape(x_tr)[1]\n",
        "  u =  w_0 or np.random.rand(c,d)\n",
        "  w =  w_0 or np.random.rand(c,d)\n",
        "  while t <= n_iter: \n",
        "    u_next = w - lr * grad_h(w,x_tr,y_tr,lam,w0)\n",
        "    w = (1 + rho) * u_next - rho * u\n",
        "    u = u_next\n",
        "    w_all.append(w)\n",
        "    t += 1 \n",
        "  return np.array(w_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVPPldwjmWk2"
      },
      "source": [
        "We can now look at the trend in training loss for different momentum values (fixed at $\\lambda = 10$). Each value of $\\rho$ will correspond to a $\\kappa$. We will use $\\rho = 0.75, 0.8, 0.85, 0.9, 0.95$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaSiCBJe0n0f"
      },
      "outputs": [],
      "source": [
        "rho = [0.75, 0.8, 0.85, 0.9, 0.95] \n",
        "lam = 10 \n",
        "lr = 0.01\n",
        "T_max = 100\n",
        "w_all_1 = train_nest(x_tr, y_tr,rho[0],lam, lr, T_max)\n",
        "w_all_2 = train_nest(x_tr, y_tr,rho[1],lam, lr, T_max)\n",
        "w_all_3 = train_nest(x_tr, y_tr,rho[2],lam, lr, T_max)\n",
        "w_all_4 = train_nest(x_tr, y_tr,rho[3],lam, lr, T_max)\n",
        "w_all_5 = train_nest(x_tr, y_tr,rho[4],lam, lr, T_max)\n",
        "loss_tr_1 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_1]\n",
        "loss_tr_2 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_2]\n",
        "loss_tr_3 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_3]\n",
        "loss_tr_4 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_4]\n",
        "loss_tr_5 = [loss(w,x_tr, y_tr, lam)[0] for w in w_all_5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Avtknftd2RJr",
        "outputId": "d2dd6b4a-579c-436f-c01c-22b61c72bd2e"
      },
      "outputs": [],
      "source": [
        "t_rng = np.array(range(len( loss_tr_1))) \n",
        "plot.semilogy(t_rng, loss_tr_1)\n",
        "plot.semilogy(t_rng, loss_tr_2)\n",
        "plot.semilogy(t_rng, loss_tr_3)\n",
        "plot.semilogy(t_rng, loss_tr_4)\n",
        "plot.semilogy(t_rng, loss_tr_5)\n",
        "plot.legend(('$p = 0.75$','$p = 0.80$','$p = 0.85$','$p = 0.90$','$p = 0.95$') )\n",
        "plot.ylabel('Training loss [log-scale]')\n",
        "plot.xlabel('Iteration number')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0xwhfRYCCoe",
        "outputId": "87bc3f2d-0d1a-4ac5-8aed-7470859cb81e"
      },
      "outputs": [],
      "source": [
        "fit_1 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_1[0:20]), 1)\n",
        "fit_2 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_2[0:20]), 1)\n",
        "fit_3 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_3[0:20]), 1)\n",
        "fit_4 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_4[0:20]), 1)\n",
        "fit_5 = z = np.polyfit(t_rng[0:20], np.log(loss_tr_5[0:20]), 1)\n",
        "print(\"Rho = 0.75:\"+\" \"+ str(fit_1[0]))\n",
        "print(\"Rho =  0.80:\"+\" \"+ str(fit_2[0]))\n",
        "print(\"Rho =  0.85:\"+\" \"+ str(fit_3[0]))\n",
        "print(\"Rho =  0.90:\"+\" \"+ str(fit_4[0]))\n",
        "print(\"Rho =  0.95:\"+\" \"+ str(fit_5[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N64x9cYG42f8"
      },
      "source": [
        "(e) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai1AXnao4ivS"
      },
      "outputs": [],
      "source": [
        "def train_sgd_minibatch(x_tr, y_tr, B, lam, lr, n_iter, w0=0,w_0 =None):\n",
        "  ##Paramaters: \n",
        "  # x_tr: data matrix (nxd array)\n",
        "  # y_tr: true label array (nx1 array)\n",
        "  # lam: regularization constant (constant)\n",
        "  # lr: learning rate \n",
        "  # n_iter: number of iteration steps \n",
        "  # w0: bias matrix (cxn array)\n",
        "  #Output: grad (cxd array)\n",
        "  c = 2   \n",
        "  n = np.shape(x_tr)[0]\n",
        "  d = np.shape(x_tr)[1]\n",
        "  w = w_0 or np.random.rand(c,d)\n",
        "  w_all = []\n",
        "  w_all.append(w)\n",
        "  t = 0 \n",
        "  while t <= n_iter:  \n",
        "    ii = np.random.randint(n, size=B)\n",
        "    x = x_tr[ii]\n",
        "    y = y_tr[ii]\n",
        "    w = w - lr*grad_h(w,x,y,lam,w0)\n",
        "    w_all.append(w)\n",
        "    t += 1 \n",
        "  return np.array(w_all)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVa5vOJU6_EO"
      },
      "outputs": [],
      "source": [
        "lam = 10 \n",
        "B = 128 \n",
        "lr = 0.01 \n",
        "T = 100\n",
        "w_all = train_sgd_minibatch(x_tr, y_tr,B,lam, lr, T)\n",
        "loss_tr = [loss(w,x_tr, y_tr, lam)[0] for w in w_all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2e5zm2U7_n7B",
        "outputId": "71c120f1-2d38-4918-9f4e-f132f19c76a6"
      },
      "outputs": [],
      "source": [
        "t_rng = np.array(range(len(w_all))) \n",
        "plot.semilogy(t_rng, loss_tr)\n",
        "plot.ylabel('Training loss [log-scale]')\n",
        "plot.xlabel('Iteration number')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2HH26XI_8CC"
      },
      "outputs": [],
      "source": [
        "def train_sgd_minibatch_nest(x_tr, y_tr, B, rho, lam, lr, n_iter, w0=0,w_0 =None):\n",
        "  ##Paramaters: \n",
        "  # x_tr: data matrix (nxd array)\n",
        "  # y_tr: true label array (nx1 array)\n",
        "  # lam: regularization constant (constant)\n",
        "  # lr: learning rate \n",
        "  # n_iter: number of iteration steps \n",
        "  # w0: bias matrix (cxn array)\n",
        "  #Output: grad (cxd array)\n",
        "  c = 2   \n",
        "  n = np.shape(x_tr)[0]\n",
        "  d = np.shape(x_tr)[1]\n",
        "  u =  w_0 or np.random.rand(c,d)\n",
        "  w =  w_0 or np.random.rand(c,d)\n",
        "  w_all = []\n",
        "  w_all.append(w)\n",
        "  t = 0 \n",
        "  while t <= n_iter:  \n",
        "    ii = np.random.randint(n, size=B)\n",
        "    x = x_tr[ii]\n",
        "    y = y_tr[ii]\n",
        "    u_next = w - lr * grad_h(w,x,y,lam,w0)\n",
        "    w = (1 + rho) * u_next - rho * u\n",
        "    u = u_next\n",
        "    w_all.append(w)\n",
        "    t += 1 \n",
        "  return np.array(w_all)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDcSe9o4ucOj"
      },
      "source": [
        "Plot of loss for 4 algorithms, with $\\rho = 0.9$. \n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoh-BZlXBHBh"
      },
      "outputs": [],
      "source": [
        "lam = 10 \n",
        "B = 128 \n",
        "lr = 0.01 \n",
        "T = 100 \n",
        "rho = 0.75 \n",
        "w_gd  = train_gd(x_tr, y_tr, lam, lr, T)\n",
        "w_nest_gd  = train_nest(x_tr, y_tr,rho,lam, lr, T)\n",
        "w_sgd = train_sgd_minibatch(x_tr, y_tr,B,lam, lr, T)\n",
        "w_nest_sgd = train_sgd_minibatch_nest(x_tr, y_tr,B, rho, lam, lr, T)\n",
        "\n",
        "loss_gd = [loss(w,x_tr, y_tr, lam)[0] for w in w_gd]\n",
        "loss_nest_gd = [loss(w,x_tr, y_tr, lam)[0] for w in w_nest_gd]\n",
        "loss_sgd = [loss(w,x_tr, y_tr, lam)[0] for w in w_sgd]\n",
        "loss_nest_sgd = [loss(w,x_tr, y_tr, lam)[0] for w in w_nest_sgd]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XnW3x6X3BWD0",
        "outputId": "5e5d889c-e821-465f-b2ee-6445948800cd"
      },
      "outputs": [],
      "source": [
        "t_rng = np.array(range(len(w_gd))) \n",
        "t_rng2 = np.array(range(len(loss_sgd))) \n",
        "plot.semilogy(t_rng, loss_gd)\n",
        "plot.semilogy(t_rng, loss_nest_gd)\n",
        "plot.semilogy(t_rng2, loss_sgd)\n",
        "plot.semilogy(t_rng2, loss_nest_sgd)\n",
        "plot.legend(('GD','GD with Nest.','SGD','SGD with Nest') )\n",
        "plot.ylabel('Training loss [log-scale]')\n",
        "plot.xlabel('Iteration number')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPgnlNMDGKcz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "leclerc_nima_hw4_prob1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
