{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79e8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F, BCELoss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9f0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \"\"\"Encoder layers\"\"\"\n",
    "        self.linear1 = nn.Linear(196, 128)\n",
    "        self.linear2 = nn.Linear(128, 16)  \n",
    "        \"\"\"Decoder layers\"\"\"\n",
    "        self.linear3 = nn.Linear(8, 128)\n",
    "        self.linear4 = nn.Linear(128, 196)\n",
    "\n",
    "    def encode(self, x):\n",
    "        layer1 = torch.tanh(self.linear1(x))\n",
    "        encoder_ouputs = self.linear2(layer1)\n",
    "        mu = encoder_ouputs[:, :8]\n",
    "        logsigma = encoder_ouputs[:, 8:]\n",
    "        return mu, logsigma\n",
    "\n",
    "    def reparameterize(self, mu, logsigma):\n",
    "        std = torch.exp(0.5*logsigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, latent_factor):\n",
    "        decodelayer1 = torch.tanh(self.linear3(latent_factor))\n",
    "        return torch.sigmoid(self.linear4(decodelayer1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logsigma = self.encode(x.view(-1, 196))\n",
    "        latent_factor = self.reparameterize(mu, logsigma)\n",
    "        return self.decode(latent_factor), mu, logsigma\n",
    "    \n",
    "def loss_function(recon_x, x, mu, logsigma):\n",
    "    BCE_Loss = F.binary_cross_entropy(recon_x, x.view(-1, 196), reduction='sum')\n",
    "    KL_Divergence = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())\n",
    "    return BCE_Loss, KL_Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97832924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 14, 14]) torch.Size([10000])\n",
      "torch.Size([10000, 14, 14]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "traindata = datasets.MNIST(root='./data', train=True, download=True)\n",
    "valdataset = datasets.MNIST(root='./data', train=False, download=True)\n",
    "balanced_train_data = []; balanced_train_targets = []\n",
    "for i in range(10):\n",
    "    labelimgs = [labelimage for labelimage, label in traindata if label == i][:1000]\n",
    "    for image in labelimgs:\n",
    "        image = (transforms.ToTensor()(image.resize((14,14)))>0.5).float()\n",
    "        balanced_train_data.append(image)\n",
    "    balanced_train_targets.append(torch.ones(len(labelimgs))*i)\n",
    "balanced_train_data = torch.cat(balanced_train_data)\n",
    "balanced_train_targets = torch.cat(balanced_train_targets)\n",
    "\n",
    "valdata = []; valtargets = []\n",
    "for image, label in valdataset:\n",
    "    image = (transforms.ToTensor()(image.resize((14,14)))>0.5).float()\n",
    "    valdata.append(image)\n",
    "    valtargets.append(torch.tensor(label))\n",
    "valdata = torch.cat(valdata)\n",
    "valtargets = torch.tensor(valtargets)\n",
    "valdataset.data = valdata\n",
    "valdataset.targets = valtargets\n",
    "print(balanced_train_data.shape, balanced_train_targets.shape)\n",
    "print(valdata.shape, valtargets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3845a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 58.812\n",
      "Epoch: 2, Loss: 44.902\n",
      "Epoch: 3, Loss: 42.149\n",
      "Epoch: 4, Loss: 40.418\n",
      "Epoch: 5, Loss: 39.297\n",
      "Epoch: 6, Loss: 38.644\n",
      "Epoch: 7, Loss: 38.247\n",
      "Epoch: 8, Loss: 37.816\n",
      "Epoch: 9, Loss: 37.519\n",
      "Epoch: 10, Loss: 37.209\n",
      "Epoch: 11, Loss: 36.900\n",
      "Epoch: 12, Loss: 36.616\n",
      "Epoch: 13, Loss: 36.378\n",
      "Epoch: 14, Loss: 36.180\n",
      "Epoch: 15, Loss: 35.985\n",
      "Epoch: 16, Loss: 35.831\n",
      "Epoch: 17, Loss: 35.663\n",
      "Epoch: 18, Loss: 35.538\n",
      "Epoch: 19, Loss: 35.410\n",
      "Epoch: 20, Loss: 35.312\n",
      "Epoch: 21, Loss: 35.179\n",
      "Epoch: 22, Loss: 35.083\n",
      "Epoch: 23, Loss: 34.990\n",
      "Epoch: 24, Loss: 34.874\n",
      "Epoch: 25, Loss: 34.834\n",
      "Epoch: 26, Loss: 34.684\n",
      "Epoch: 27, Loss: 34.639\n",
      "Epoch: 28, Loss: 34.572\n",
      "Epoch: 29, Loss: 34.479\n",
      "Epoch: 30, Loss: 34.432\n",
      "Epoch: 31, Loss: 34.380\n",
      "Epoch: 32, Loss: 34.294\n",
      "Epoch: 33, Loss: 34.256\n",
      "Epoch: 34, Loss: 34.224\n",
      "Epoch: 35, Loss: 34.110\n",
      "Epoch: 36, Loss: 34.073\n",
      "Epoch: 37, Loss: 33.985\n",
      "Epoch: 38, Loss: 33.994\n",
      "Epoch: 39, Loss: 33.863\n",
      "Epoch: 40, Loss: 33.905\n",
      "Epoch: 41, Loss: 33.803\n",
      "Epoch: 42, Loss: 33.796\n",
      "Epoch: 43, Loss: 33.756\n",
      "Epoch: 44, Loss: 33.707\n",
      "Epoch: 45, Loss: 33.645\n",
      "Epoch: 46, Loss: 33.621\n",
      "Epoch: 47, Loss: 33.591\n",
      "Epoch: 48, Loss: 33.535\n",
      "Epoch: 49, Loss: 33.515\n"
     ]
    }
   ],
   "source": [
    "VariationalAE = VAE()\n",
    "optimizer = torch.optim.Adam(VariationalAE.parameters(), lr = 0.001, weight_decay=1e-5)\n",
    "train_losses = []\n",
    "BCE_losses = []\n",
    "KLD_losses = []\n",
    "all_val_losses = []\n",
    "epochs = 50\n",
    "dataloader = DataLoader(list(zip(balanced_train_data, balanced_train_targets)), \n",
    "                        batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(valdata, batch_size=100, shuffle=True)\n",
    "\n",
    "def validate(VariationalAE, dataloader):\n",
    "    VariationalAE.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in (dataloader):\n",
    "            data = batch[0]\n",
    "            recon_batch, mu, logvar = VariationalAE(data)\n",
    "            BCE, KLD = loss_function(recon_batch, data, mu, logvar)\n",
    "            val_loss += BCE.item() + KLD.item()\n",
    "    return val_loss / len(dataloader.dataset)\n",
    "\n",
    "weight_update = 0\n",
    "for epoch in range(epochs):\n",
    "    VariationalAE.train()\n",
    "    train_loss = 0\n",
    "    for _, (data, _) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = VariationalAE(data)\n",
    "        BCE_loss, KL_Divergence = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = BCE_loss + KL_Divergence\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        BCE_losses.append(BCE_loss.item())\n",
    "        KLD_losses.append(KL_Divergence.item())\n",
    "\n",
    "        weight_update += 1\n",
    "        if weight_update % 100 == 0:\n",
    "            val_loss = validate(VariationalAE, val_dataloader)\n",
    "            all_val_losses.append(val_loss/len(val_dataloader))\n",
    "    print(f'Epoch: {epoch+1}, Loss: {train_loss / len(dataloader.dataset):.3f}')\n",
    "    train_losses.append(train_loss / len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style(\"whitegrid\")  # Set seaborn style\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss for the VAE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(all_val_losses)\n",
    "plt.title(\"Validation Loss for the VAE\")\n",
    "plt.xlabel(\"Weight Update\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Training_Validation_Loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290007f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(BCE_losses, label='Reconstruction Loss', alpha=0.9)\n",
    "plt.plot(pd.Series(BCE_losses).rolling(window=100).mean(), label='Moving Average')\n",
    "plt.title(\"Reconstruction Loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(KLD_losses, label='KL Divergence', alpha=0.9)\n",
    "plt.plot(pd.Series(KLD_losses).rolling(window=100).mean(), label='Moving Average')\n",
    "plt.title(\"KL Divergence\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloader))\n",
    "images = images[:8]; labels = labels[:8]\n",
    "\n",
    "generated_images, _, _ = VariationalAE(images)\n",
    "images = images.detach().numpy()\n",
    "generated_images = generated_images.detach().numpy()\n",
    "\n",
    "# Plot the original and reconstructed images side by side\n",
    "fig, axs = plt.subplots(4, 4, figsize=(17, 16))\n",
    "for i in range(8):\n",
    "    # Original images\n",
    "    axs[i//4, i%4].imshow(images[i].reshape(14, 14), cmap='gray')\n",
    "    axs[i//4, i%4].set_title(f'Original {int(labels[i])}')\n",
    "    axs[i//4, i%4].axis('off')\n",
    "    # Reconstructed images\n",
    "    axs[(i+8)//4, (i+8)%4].imshow(generated_images[i].reshape(14, 14), cmap='gray')\n",
    "    axs[(i+8)//4, (i+8)%4].set_title(f'Generated {int(labels[i])}')\n",
    "    axs[(i+8)//4, (i+8)%4].axis('off')\n",
    "\n",
    "plt.subplots_adjust(top=0.9)  # Adjust the top position of the subplots\n",
    "plt.suptitle(\"Original vs Generated Images\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Original_vs_Generated.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sampling from the latent space\"\"\"\n",
    "fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Sample z from a standard Gaussian distribution\n",
    "z = torch.randn(8, 8)\n",
    "\n",
    "# Run the decoder network to synthesize an image\n",
    "syn_images = VariationalAE.decode(z)\n",
    "\n",
    "# Convert the synthesized images to numpy arrays\n",
    "syn_images = syn_images.detach().numpy()\n",
    "\n",
    "# Plot the synthesized images\n",
    "for i in range(8):\n",
    "    axs[i//4, i%4].imshow(syn_images[i].reshape(14, 14), cmap='gray')\n",
    "    axs[i//4, i%4].axis('off')\n",
    "\n",
    "plt.suptitle(\"Sampling from the latent space\")\n",
    "plt.savefig(\"Sampling_from_latent_space.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(valdata, batch_size=100, shuffle=True)\n",
    "\n",
    "def validate(VariationalAE, dataloader):\n",
    "    VariationalAE.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in (dataloader):\n",
    "            data = batch[0]\n",
    "            recon_batch, mu, logvar = VariationalAE(data)\n",
    "            BCE, KLD = loss_function(recon_batch, data, mu, logvar)\n",
    "            val_loss += BCE.item() + KLD.item()\n",
    "    return val_loss / len(dataloader.dataset)\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "update_count = 0\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    for batch in (dataloader):\n",
    "        data = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = VariationalAE(data)\n",
    "        BCE, KLD = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = BCE + KLD\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        all_train_losses.append(BCE.item()/len(dataloader))\n",
    "        \n",
    "        update_count += 1\n",
    "        if update_count % 100 == 0:\n",
    "            val_loss = validate(VariationalAE, val_dataloader)\n",
    "            all_val_losses.append(val_loss/len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(all_train_losses[:-50], label='Train Loss')  # Exclude the last value\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Weight Update\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(0, len(all_train_losses[:-50]), 100), all_val_losses, label='Validation Loss')\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.xlabel(\"Weight Update\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle(\"Training and Validation Loss\")\n",
    "plt.savefig(\"Training_Validation_Loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402bfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
